{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# get boilerplate\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "#make the depreciation warnings go away\n",
    "import warnings\n",
    "#I'm tired of the warnings on functions the professor asks us to use :) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random acts of Pizza\n",
    "### from https://www.reddit.com/r/Random_Acts_Of_Pizza/\n",
    "### Purpose of Random Acts of Pizza:\n",
    "###\n",
    "###The Original Random Pizza Delivery Service\n",
    "### We are open for active Redditors that have not engaged in questionable online \n",
    "### behavior (relax, we don't know about all of it, you're probably fine). See The \n",
    "### Pizza Library for everything you need to know to get started.\n",
    "### https://www.reddit.com/r/Random_Acts_Of_Pizza/wiki/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4040"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grab a practice set\n",
    "prac = d[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['requester_subreddits_at_request', 'requester_number_of_posts_on_raop_at_request', 'request_title', 'requester_number_of_posts_on_raop_at_retrieval', 'post_was_edited', 'requester_upvotes_minus_downvotes_at_retrieval', 'requester_number_of_comments_at_retrieval', 'unix_timestamp_of_request_utc', 'requester_account_age_in_days_at_request', 'requester_account_age_in_days_at_retrieval', 'requester_number_of_comments_in_raop_at_request', 'request_id', 'requester_username', 'requester_number_of_subreddits_at_request', 'unix_timestamp_of_request', 'giver_username_if_known', 'requester_number_of_comments_in_raop_at_retrieval', 'number_of_upvotes_of_request_at_retrieval', 'requester_number_of_posts_at_request', 'request_text_edit_aware', 'requester_upvotes_plus_downvotes_at_request', 'request_number_of_comments_at_retrieval', 'requester_number_of_posts_at_retrieval', 'requester_received_pizza', 'requester_days_since_first_post_on_raop_at_request', 'requester_number_of_comments_at_request', 'requester_upvotes_minus_downvotes_at_request', 'requester_upvotes_plus_downvotes_at_retrieval', 'request_text', 'requester_user_flair', 'requester_days_since_first_post_on_raop_at_retrieval', 'number_of_downvotes_of_request_at_retrieval'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_l25d7',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_title': 'Request Colorado Springs Help Us Please',\n",
       " 'requester_account_age_in_days_at_request': 0.0,\n",
       " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 0,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 0,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': [],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'nickylvst',\n",
       " 'unix_timestamp_of_request': 1317852607.0,\n",
       " 'unix_timestamp_of_request_utc': 1317849007.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "      <th>requester_subreddits_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_minus_downvotes_at_retrieval</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_retrieval</th>\n",
       "      <th>requester_user_flair</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.111100</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, Eve, IAmA, MontereyBay, RandomKind...</td>\n",
       "      <td>34</td>\n",
       "      <td>4258</td>\n",
       "      <td>116</td>\n",
       "      <td>11168</td>\n",
       "      <td>None</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>jacquibatman7</td>\n",
       "      <td>1319650094</td>\n",
       "      <td>1319646494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>4</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>6.518438</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]</td>\n",
       "      <td>54</td>\n",
       "      <td>59</td>\n",
       "      <td>76</td>\n",
       "      <td>81</td>\n",
       "      <td>None</td>\n",
       "      <td>4on_the_floor</td>\n",
       "      <td>1322855434</td>\n",
       "      <td>1322855434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>5</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>162.063252</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>[GayBrosWeightLoss, RandomActsOfCookies, Rando...</td>\n",
       "      <td>1121</td>\n",
       "      <td>1225</td>\n",
       "      <td>1733</td>\n",
       "      <td>1887</td>\n",
       "      <td>None</td>\n",
       "      <td>Futuredogwalker</td>\n",
       "      <td>1373657691</td>\n",
       "      <td>1373654091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "2                     N/A                                            0   \n",
       "3                     N/A                                            0   \n",
       "4                     N/A                                            6   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "2                                          3                0   t3_lpu5j   \n",
       "3                                          1                1   t3_mxvj3   \n",
       "4                                          6                0  t3_1i6486   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        4   \n",
       "4                                        5   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "4  hey guys:\\n I love this sub. I think it's grea...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "4  hey guys:\\n I love this sub. I think it's grea...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "2  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "   requester_account_age_in_days_at_request              ...                \\\n",
       "0                                  0.000000              ...                 \n",
       "1                                501.111100              ...                 \n",
       "2                                  0.000000              ...                 \n",
       "3                                  6.518438              ...                 \n",
       "4                                162.063252              ...                 \n",
       "\n",
       "   requester_received_pizza  \\\n",
       "0                     False   \n",
       "1                     False   \n",
       "2                     False   \n",
       "3                     False   \n",
       "4                     False   \n",
       "\n",
       "                     requester_subreddits_at_request  \\\n",
       "0                                                 []   \n",
       "1  [AskReddit, Eve, IAmA, MontereyBay, RandomKind...   \n",
       "2                                                 []   \n",
       "3       [AskReddit, DJs, IAmA, Random_Acts_Of_Pizza]   \n",
       "4  [GayBrosWeightLoss, RandomActsOfCookies, Rando...   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_request  \\\n",
       "0                                             0   \n",
       "1                                            34   \n",
       "2                                             0   \n",
       "3                                            54   \n",
       "4                                          1121   \n",
       "\n",
       "   requester_upvotes_minus_downvotes_at_retrieval  \\\n",
       "0                                               1   \n",
       "1                                            4258   \n",
       "2                                               3   \n",
       "3                                              59   \n",
       "4                                            1225   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request  \\\n",
       "0                                            0   \n",
       "1                                          116   \n",
       "2                                            0   \n",
       "3                                           76   \n",
       "4                                         1733   \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_retrieval  requester_user_flair  \\\n",
       "0                                              1                  None   \n",
       "1                                          11168                  None   \n",
       "2                                              3                  None   \n",
       "3                                             81                  None   \n",
       "4                                           1887                  None   \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "2       jacquibatman7                 1319650094   \n",
       "3       4on_the_floor                 1322855434   \n",
       "4     Futuredogwalker                 1373657691   \n",
       "\n",
       "   unix_timestamp_of_request_utc  \n",
       "0                     1317849007  \n",
       "1                     1332648824  \n",
       "2                     1319646494  \n",
       "3                     1322855434  \n",
       "4                     1373654091  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['giver_username_if_known', 'request_id', 'request_text_edit_aware',\n",
       "       'request_title', 'requester_account_age_in_days_at_request',\n",
       "       'requester_days_since_first_post_on_raop_at_request',\n",
       "       'requester_number_of_comments_at_request',\n",
       "       'requester_number_of_comments_in_raop_at_request',\n",
       "       'requester_number_of_posts_at_request',\n",
       "       'requester_number_of_posts_on_raop_at_request',\n",
       "       'requester_number_of_subreddits_at_request',\n",
       "       'requester_subreddits_at_request',\n",
       "       'requester_upvotes_minus_downvotes_at_request',\n",
       "       'requester_upvotes_plus_downvotes_at_request', 'requester_username',\n",
       "       'unix_timestamp_of_request', 'unix_timestamp_of_request_utc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac[0]['requester_received_pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac[4]['requester_received_pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### get the labels from the dictionaries\n",
    "labs = []\n",
    "for req in prac:\n",
    "    if req['requester_received_pizza'] == True: print(1)\n",
    "    else: print(0)\n",
    "labs = [1 if x['requester_received_pizza'] == True else 0 for x in prac]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 3,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 4,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_14gmeb',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': \"Feeling under the weather so I called out off work today! I hate requesting because I feel like I'm begging so I thought I'd give back! \\n\\n(I'd offer pizza if today were payday :C)\",\n",
       " 'request_text_edit_aware': \"Feeling under the weather so I called out off work today! I hate requesting because I feel like I'm begging so I thought I'd give back! \\n\\n(I'd offer pizza if today were payday :C)\",\n",
       " 'request_title': \"[REQUEST] I'll give a two week xbox live code for a slice of pie!\",\n",
       " 'requester_account_age_in_days_at_request': 582.7765856481482,\n",
       " 'requester_account_age_in_days_at_retrieval': 946.2708796296296,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 340.8193287037037,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 704.3136226851852,\n",
       " 'requester_number_of_comments_at_request': 63,\n",
       " 'requester_number_of_comments_at_retrieval': 68,\n",
       " 'requester_number_of_comments_in_raop_at_request': 1,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 1,\n",
       " 'requester_number_of_posts_at_request': 24,\n",
       " 'requester_number_of_posts_at_retrieval': 30,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 21,\n",
       " 'requester_received_pizza': True,\n",
       " 'requester_subreddits_at_request': ['AdviceAnimals',\n",
       "  'AskReddit',\n",
       "  'Autos',\n",
       "  'IAmA',\n",
       "  'RandomKindness',\n",
       "  'Random_Acts_Of_Pizza',\n",
       "  'TheFacebookDelusion',\n",
       "  'atheism',\n",
       "  'aww',\n",
       "  'cars',\n",
       "  'coupons',\n",
       "  'fffffffuuuuuuuuuuuu',\n",
       "  'funny',\n",
       "  'gaming',\n",
       "  'highdesert',\n",
       "  'loseit',\n",
       "  'pics',\n",
       "  'reddit.com',\n",
       "  'shittybattlestations',\n",
       "  'todayilearned',\n",
       "  'videos'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 234,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 533,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 814,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1207,\n",
       " 'requester_user_flair': 'shroom',\n",
       " 'requester_username': 'jamespweb',\n",
       " 'unix_timestamp_of_request': 1354911700.0,\n",
       " 'unix_timestamp_of_request_utc': 1354911700.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-12-07 12:21:40\n"
     ]
    }
   ],
   "source": [
    "### maybe requests have to do with the time of day\n",
    "import datetime\n",
    "print(\n",
    "    datetime.datetime.fromtimestamp(\n",
    "        int(1354911700.0)\n",
    "    ).strftime('%Y-%m-%d %H:%M:%S')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'human_readable_UTC_time': '2011-10-05 14:10:07',\n",
       " 'human_readable_local_time': '2011-10-05 15:10:07',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_l25d7',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_title': 'Request Colorado Springs Help Us Please',\n",
       " 'requester_account_age_in_days_at_request': 0.0,\n",
       " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 0,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 0,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': [],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'nickylvst',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1317852607.0,\n",
       " 'unix_timestamp_of_request_utc': 1317849007.0,\n",
       " 'weekday': 'Wednesday'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get human readible times\n",
    "\n",
    "def day_time(x):\n",
    "    y = ''\n",
    "    if datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 10:\n",
    "        y = 'morning'\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 10 & datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 16:\n",
    "        y = 'midday'\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 16 & datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 21:\n",
    "        y = 'evening'\n",
    "    else: \n",
    "        y = 'night'\n",
    "    return y\n",
    "        \n",
    "def human_time(a):\n",
    "    import datetime\n",
    "    from datetime import date\n",
    "    import calendar\n",
    "    ### for the data in raop, return human time.  maybe the time of day matters\n",
    "    a['human_readable_local_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['human_readable_UTC_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    #a['weekday'] = date(fromtimestamp(a['human_readable_UTC_time'])).weekday()\n",
    "    a['weekday'] = calendar.day_name[datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).weekday()]\n",
    "    a['time_of_date'] = day_time(a)\n",
    "    return a\n",
    "\n",
    "prac1 = [human_time(x) for x in prac]\n",
    "\n",
    "prac1[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'human_readable_UTC_time': '2012-03-24 21:13:44',\n",
       " 'human_readable_local_time': '2012-03-24 22:13:44',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 2,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 5,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_rcb83',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'I spent the last money I had on gas today. Im broke until next Thursday :(',\n",
       " 'request_text_edit_aware': 'I spent the last money I had on gas today. Im broke until next Thursday :(',\n",
       " 'request_title': '[Request] California, No cash and I could use some dinner',\n",
       " 'requester_account_age_in_days_at_request': 501.11109953703703,\n",
       " 'requester_account_age_in_days_at_retrieval': 1122.279837962963,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 621.1270717592593,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 1000,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 15,\n",
       " 'requester_number_of_posts_at_retrieval': 26,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 2,\n",
       " 'requester_number_of_subreddits_at_request': 12,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': ['AskReddit',\n",
       "  'Eve',\n",
       "  'IAmA',\n",
       "  'MontereyBay',\n",
       "  'RandomKindness',\n",
       "  'RedditBiography',\n",
       "  'dubstep',\n",
       "  'gamecollecting',\n",
       "  'gaming',\n",
       "  'halo',\n",
       "  'i18n',\n",
       "  'techsupport'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 34,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 4258,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 116,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 11168,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'fohacidal',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1332652424.0,\n",
       " 'unix_timestamp_of_request_utc': 1332648824.0,\n",
       " 'weekday': 'Saturday'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prac1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'human_readable_UTC_time': '2012-12-07 12:21:40',\n",
       " 'human_readable_local_time': '2012-12-07 12:21:40',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 3,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 4,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_14gmeb',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': \"Feeling under the weather so I called out off work today! I hate requesting because I feel like I'm begging so I thought I'd give back! \\n\\n(I'd offer pizza if today were payday :C)\",\n",
       " 'request_text_edit_aware': \"Feeling under the weather so I called out off work today! I hate requesting because I feel like I'm begging so I thought I'd give back! \\n\\n(I'd offer pizza if today were payday :C)\",\n",
       " 'request_title': \"[REQUEST] I'll give a two week xbox live code for a slice of pie!\",\n",
       " 'requester_account_age_in_days_at_request': 582.7765856481482,\n",
       " 'requester_account_age_in_days_at_retrieval': 946.2708796296296,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 340.8193287037037,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 704.3136226851852,\n",
       " 'requester_number_of_comments_at_request': 63,\n",
       " 'requester_number_of_comments_at_retrieval': 68,\n",
       " 'requester_number_of_comments_in_raop_at_request': 1,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 1,\n",
       " 'requester_number_of_posts_at_request': 24,\n",
       " 'requester_number_of_posts_at_retrieval': 30,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 21,\n",
       " 'requester_received_pizza': True,\n",
       " 'requester_subreddits_at_request': ['AdviceAnimals',\n",
       "  'AskReddit',\n",
       "  'Autos',\n",
       "  'IAmA',\n",
       "  'RandomKindness',\n",
       "  'Random_Acts_Of_Pizza',\n",
       "  'TheFacebookDelusion',\n",
       "  'atheism',\n",
       "  'aww',\n",
       "  'cars',\n",
       "  'coupons',\n",
       "  'fffffffuuuuuuuuuuuu',\n",
       "  'funny',\n",
       "  'gaming',\n",
       "  'highdesert',\n",
       "  'loseit',\n",
       "  'pics',\n",
       "  'reddit.com',\n",
       "  'shittybattlestations',\n",
       "  'todayilearned',\n",
       "  'videos'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 234,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 533,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 814,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1207,\n",
       " 'requester_user_flair': 'shroom',\n",
       " 'requester_username': 'jamespweb',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1354911700.0,\n",
       " 'unix_timestamp_of_request_utc': 1354911700.0,\n",
       " 'weekday': 'Friday'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac1[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'human_readable_UTC_time': '2011-08-31 13:59:42',\n",
       " 'human_readable_local_time': '2011-08-31 14:59:42',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 6,\n",
       " 'post_was_edited': True,\n",
       " 'request_id': 't3_k0l9j',\n",
       " 'request_number_of_comments_at_retrieval': 21,\n",
       " 'request_text': \"Austin, Texas\\n\\nMy two roommates and I are hungry as hell. We were all sort of counting on the deposit from our last apartment to help us out, but they claimed there was damages, so we did not receive anything. So, we're sort of struggling. Is anyone able to help us out for dinner tonight? We'd really appreciate it!\\n\\nEDIT: Received a Dominos gift card from jetboyterp! THANK YOU!\",\n",
       " 'request_text_edit_aware': \"Austin, Texas\\n\\nMy two roommates and I are hungry as hell. We were all sort of counting on the deposit from our last apartment to help us out, but they claimed there was damages, so we did not receive anything. So, we're sort of struggling. Is anyone able to help us out for dinner tonight? We'd really appreciate it!\\n\",\n",
       " 'request_title': \"[REQUEST]We're in need of some om noms...\",\n",
       " 'requester_account_age_in_days_at_request': 348.92042824074076,\n",
       " 'requester_account_age_in_days_at_retrieval': 1176.38875,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 827.4266550925926,\n",
       " 'requester_number_of_comments_at_request': 2,\n",
       " 'requester_number_of_comments_at_retrieval': 11,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 9,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 1,\n",
       " 'requester_received_pizza': True,\n",
       " 'requester_subreddits_at_request': ['AskReddit'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 6,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 26,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 6,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 28,\n",
       " 'requester_user_flair': 'shroom',\n",
       " 'requester_username': 'biffle',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1314827982.0,\n",
       " 'unix_timestamp_of_request_utc': 1314824382.0,\n",
       " 'weekday': 'Wednesday'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac1[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'human_readable_UTC_time': '2013-01-18 17:34:46',\n",
       " 'human_readable_local_time': '2013-01-18 17:34:46',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 4,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 5,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_16upcl',\n",
       " 'request_number_of_comments_at_retrieval': 14,\n",
       " 'request_text': \"I've been unemployed but working odd jobs. I worked a job yesterday which I thought paid up front - it did not. Now I'm sitting at home dreaming of pizza (literally, I took a nap and dreamt there was pizza in my fridge, but woke up to nothing but leftover pasta). Will pay forward when I get a chance!\",\n",
       " 'request_text_edit_aware': \"I've been unemployed but working odd jobs. I worked a job yesterday which I thought paid up front - it did not. Now I'm sitting at home dreaming of pizza (literally, I took a nap and dreamt there was pizza in my fridge, but woke up to nothing but leftover pasta). Will pay forward when I get a chance!\",\n",
       " 'request_title': '[REQUEST] Bummed out in Chicago. Too broke to go out on a Friday night. Pizza would definitely cheer me up.',\n",
       " 'requester_account_age_in_days_at_request': 52.30877314814815,\n",
       " 'requester_account_age_in_days_at_retrieval': 373.5859375,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 321.27716435185187,\n",
       " 'requester_number_of_comments_at_request': 93,\n",
       " 'requester_number_of_comments_at_retrieval': 997,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 5,\n",
       " 'requester_number_of_posts_at_request': 2,\n",
       " 'requester_number_of_posts_at_retrieval': 4,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 18,\n",
       " 'requester_received_pizza': True,\n",
       " 'requester_subreddits_at_request': ['4chan',\n",
       "  'AskReddit',\n",
       "  'IAmA',\n",
       "  'ImGoingToHellForThis',\n",
       "  'Music',\n",
       "  'Random_Acts_Of_Pizza',\n",
       "  'RealGirls',\n",
       "  'WTF',\n",
       "  'aww',\n",
       "  'community',\n",
       "  'funny',\n",
       "  'mildlyinteresting',\n",
       "  'movies',\n",
       "  'news',\n",
       "  'pics',\n",
       "  'todayilearned',\n",
       "  'trees',\n",
       "  'worldnews'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 1738,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 18617,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 2634,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 29755,\n",
       " 'requester_user_flair': 'shroom',\n",
       " 'requester_username': 'seabass86',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1358559286.0,\n",
       " 'unix_timestamp_of_request_utc': 1358559286.0,\n",
       " 'weekday': 'Friday'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac1[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'human_readable_UTC_time': '2012-11-24 22:31:33',\n",
       " 'human_readable_local_time': '2012-11-24 22:31:33',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 2,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 4,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_13r18s',\n",
       " 'request_number_of_comments_at_retrieval': 3,\n",
       " 'request_text': 'It would be much appreciated, and payed forward later on. ',\n",
       " 'request_text_edit_aware': 'It would be much appreciated, and payed forward later on. ',\n",
       " 'request_title': '[request] Cookeville, TN. My dog recently died and I could use a pizza to cheer my family &amp; me up. ',\n",
       " 'requester_account_age_in_days_at_request': 66.15856481481481,\n",
       " 'requester_account_age_in_days_at_retrieval': 442.22925925925927,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 56.08336805555555,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 432.1540625,\n",
       " 'requester_number_of_comments_at_request': 8,\n",
       " 'requester_number_of_comments_at_retrieval': 11,\n",
       " 'requester_number_of_comments_in_raop_at_request': 2,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 4,\n",
       " 'requester_number_of_posts_at_request': 2,\n",
       " 'requester_number_of_posts_at_retrieval': 3,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 4,\n",
       " 'requester_received_pizza': True,\n",
       " 'requester_subreddits_at_request': ['AskReddit',\n",
       "  'Random_Acts_Of_Pizza',\n",
       "  'aww',\n",
       "  'trees'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 11,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 16,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 31,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 36,\n",
       " 'requester_user_flair': 'shroom',\n",
       " 'requester_username': 'Transparent_Taco',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1353825093.0,\n",
       " 'unix_timestamp_of_request_utc': 1353825093.0,\n",
       " 'weekday': 'Saturday'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac1[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'ded_reckoning',\n",
       " 'human_readable_UTC_time': '2012-09-11 11:41:38',\n",
       " 'human_readable_local_time': '2012-09-11 12:41:38',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 1,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 6,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_zpzly',\n",
       " 'request_number_of_comments_at_retrieval': 7,\n",
       " 'request_text': \"-I'm located in Virginia. It would be great to have some pizza tonight, feeling under the weather. I don't get paid until the weekend, and this would make our day. \",\n",
       " 'request_text_edit_aware': \"-I'm located in Virginia. It would be great to have some pizza tonight, feeling under the weather. I don't get paid until the weekend, and this would make our day. \",\n",
       " 'request_title': \"[Request] Virginia. Girlfriend and I our sick, don't get paid until Friday, and would love a pizza. Willing to pay it forward.\",\n",
       " 'requester_account_age_in_days_at_request': 183.93306712962962,\n",
       " 'requester_account_age_in_days_at_retrieval': 634.4973148148148,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 450.5225810185185,\n",
       " 'requester_number_of_comments_at_request': 50,\n",
       " 'requester_number_of_comments_at_retrieval': 86,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 3,\n",
       " 'requester_number_of_posts_at_request': 5,\n",
       " 'requester_number_of_posts_at_retrieval': 13,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 2,\n",
       " 'requester_number_of_subreddits_at_request': 15,\n",
       " 'requester_received_pizza': True,\n",
       " 'requester_subreddits_at_request': ['AskReddit',\n",
       "  'IAmA',\n",
       "  'Rateme',\n",
       "  'StopSelfHarm',\n",
       "  'TEFL',\n",
       "  'WTF',\n",
       "  'answers',\n",
       "  'beermoney',\n",
       "  'comicbooks',\n",
       "  'confession',\n",
       "  'facepalm',\n",
       "  'financialindependence',\n",
       "  'funny',\n",
       "  'sex',\n",
       "  'sobernauts'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 105,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 327,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 179,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 519,\n",
       " 'requester_user_flair': 'shroom',\n",
       " 'requester_username': 'tapedeckghost',\n",
       " 'time_of_date': 'midday',\n",
       " 'unix_timestamp_of_request': 1347392498.0,\n",
       " 'unix_timestamp_of_request_utc': 1347388898.0,\n",
       " 'weekday': 'Tuesday'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prac1[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### We can use these transformations later for a robust ML project, but for now let's \n",
    "### extract the original text and if they succeeded in getting a pizza out of the deal\n",
    "dat = [{k: d[x][k] for k in ('requester_received_pizza', 'request_text')} for x in range(len(d))]\n",
    "\n",
    "#dat = dict((k, d[k]) for k in ('requester_received_pizza', 'request_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_l25d7'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]['request_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       "  'requester_received_pizza': False},\n",
       " {'request_text': 'I spent the last money I had on gas today. Im broke until next Thursday :(',\n",
       "  'requester_received_pizza': False},\n",
       " {'request_text': \"My girlfriend decided it would be a good idea to get off at Perth bus station when she was coming to visit me and has since had to spend all her money on a taxi to get to me here in Dundee. Any chance some kind soul would get us some pizza since we don't have any cash anymore?\",\n",
       "  'requester_received_pizza': False},\n",
       " {'request_text': \"It's cold, I'n hungry, and to be completely honest I'm broke. My mum said we're having leftovers for dinner. A random pizza arriving would be nice.\\n\\nEdit: We had leftovers.\",\n",
       "  'requester_received_pizza': False},\n",
       " {'request_text': \"hey guys:\\n I love this sub. I think it's great. (Except the sob stories. I miss when this place was fun!) Anywho, I've given a pizza out before so thought I would try my luck at getting one. My friend, who lives an hour away and our schedules do not let us see each other too much, decided to come down and visit me for the night! I would love to be able to be a good host and order her a pizza to go with some beer!\\n\\nAgain, no sob story. Just looking to share a pizza with an old friend :)\",\n",
       "  'requester_received_pizza': False}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_prac = dat[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = DictVectorizer(sparse=False)\n",
    "dat_t = v.fit_transform(dat_prac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-f52cfa71cb78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat_prac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "scipy.sparse(dat_prac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this one works\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df.request_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'I spent the last money I had on gas today. Im broke until next Thursday :(',\n",
       " \"My girlfriend decided it would be a good idea to get off at Perth bus station when she was coming to visit me and has since had to spend all her money on a taxi to get to me here in Dundee. Any chance some kind soul would get us some pizza since we don't have any cash anymore?\",\n",
       " \"It's cold, I'n hungry, and to be completely honest I'm broke. My mum said we're having leftovers for dinner. A random pizza arriving would be nice.\\n\\nEdit: We had leftovers.\",\n",
       " \"hey guys:\\n I love this sub. I think it's great. (Except the sob stories. I miss when this place was fun!) Anywho, I've given a pizza out before so thought I would try my luck at getting one. My friend, who lives an hour away and our schedules do not let us see each other too much, decided to come down and visit me for the night! I would love to be able to be a good host and order her a pizza to go with some beer!\\n\\nAgain, no sob story. Just looking to share a pizza with an old friend :)\"]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.request_text.tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get labels\n",
    "Y_train_counts = np.array(df.requester_received_pizza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b =np.random.shuffle(prac1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = X_train_counts[int(X_train_counts.shape[0]/2):], Y_train_counts[int(Y_train_counts.shape[0]/2):]\n",
    "dev_data, dev_labels = X_train_counts[int(X_train_counts.shape[0]/2):], Y_train_counts[int(Y_train_counts.shape[0]/2):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2020, 12593)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2020, 12593)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-293f535a133f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#part A What is the size of the vocabulary? What is the average number of non-zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#features per example? What fraction of the entries in the matrix are non-zero?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "tr = test_data\n",
    "d = count_vect.fit_transform(test_data)\n",
    "#part A What is the size of the vocabulary? What is the average number of non-zero\n",
    "#features per example? What fraction of the entries in the matrix are non-zero?\n",
    "\n",
    "def nzf(tr):\n",
    "    #s = d.shape[1]\n",
    "    avgs = []\n",
    "    for art in tr:\n",
    "        nonzero_feat = 0\n",
    "        for term in v.get_feature_names():\n",
    "            if term in art:\n",
    "                #print(term)\n",
    "                nonzero_feat +=1\n",
    "            else: #print(term, \"not in article\")\n",
    "                continue\n",
    "        try: avgs.append(nonzero_feat)\n",
    "        except: 0\n",
    "    #this is an average of averages, change that\n",
    "    return np.average(avgs)\n",
    "\n",
    "def nzf2(tr):\n",
    "    avgs = []\n",
    "    for art in tr:\n",
    "        nonzero_feat = len(set(art.lower().split()))\n",
    "        avgs.append(nonzero_feat)\n",
    "    return np.average(avgs)\n",
    "        \n",
    "        \n",
    "\n",
    "def porp(m):\n",
    "    m1 = m.toarray()\n",
    "    tot_size = m.shape[0] * m.shape[1]\n",
    "    x = 0\n",
    "    for i in range(m.shape[0]):\n",
    "        x += m.shape[1] - np.count_nonzero(m1[i] == 0)\n",
    "    return x/tot_size\n",
    "\n",
    "print(\"Part A:  The sparse matrix contains\", d.shape[1], \"words in its vocabulary.\")\n",
    "print(\"Part A: The average number of non-zero features per example is\", nzf2(tr))\n",
    "print(\"Part A: What fraction of entries in the matrix are non-zero?\", porp(d) )\n",
    "\n",
    "#part B\n",
    "\n",
    "#What are the 0th and last feature strings (in alphabetical order)? \n",
    "#Hint: use the vectorizer's get_feature_names function.\n",
    "print(\"Part B: The first feature in alphabetical order is '%s' and the last is '%s'.\" \n",
    "      % (count_vect.get_feature_names()[0], count_vect.get_feature_names()[-1]))\n",
    "\n",
    "#part C\n",
    "#Specify your own vocabulary with 4 words: [\"atheism\", \"graphics\", \"space\", \"religion\"]. \n",
    "#Confirm the training vectors are appropriately shaped. Now what's the average number of \n",
    "#non-zero features per example?\n",
    "c = [False, True]\n",
    "#c =  [\"atheism\", \"graphics\", \"space\", \"religion\"]\n",
    "#c = ['candy', 'dog']\n",
    "def ptc(trv, voc):\n",
    "    avgs = []\n",
    "    for art in trv:\n",
    "        nonzero_feat = 0\n",
    "        for word in voc:\n",
    "            if word in art:\n",
    "                nonzero_feat += 1\n",
    "            else: continue\n",
    "        avgs.append(nonzero_feat)\n",
    "    return np.average(avgs)\n",
    "\n",
    "print('Part C: The average number of non-zero features using the parsed down vocabulary of %s is %s.' % (c, ptc(tr, c)))\n",
    "\n",
    "#part D: Instead of extracting unigram word features, use \"analyzer\" and \"ngram_range\" \n",
    "#to extract bigram and trigram character features. What size vocabulary does this yield?\n",
    "\n",
    "cv = CountVectorizer(analyzer='char_wb', ngram_range=(2,3))\n",
    "dd = cv.fit_transform(tr)\n",
    "print('Part D: When using bi- and trigrams, the vocabulary has %s features.' % (dd.shape[1]))\n",
    "\n",
    "#e. Use the \"min_df\" argument to prune words that appear in fewer than 10 documents.\n",
    "#What size vocabulary does this yield?\n",
    "\n",
    "ev = CountVectorizer(min_df= 10)\n",
    "ed = ev.fit_transform(tr)\n",
    "print('Part E: When using a a term fequency of at least 10, the vocabulary has %s features.' %(ed.shape[1]))\n",
    "\n",
    "\n",
    "#f.f. Using the standard CountVectorizer, what fraction of the words in the dev data are missing from the vocabulary? \n",
    "#Hint: build a vocabulary for both train and dev and look at the size of the difference.\n",
    "v1 = CountVectorizer()\n",
    "\n",
    "fd = v1.fit_transform(dev_data)\n",
    "# set theory, find (size(set(dev)) - (size(set(train AND dev)))/size(set(dev))\n",
    "\n",
    "fl = (len(set(v1.vocabulary_)) - len(set(set(v1.vocabulary_)&set(v.vocabulary_))))/len(set(v1.vocabulary_))\n",
    "print('Part F: The porportion of development features not found in the training features is %s.' %fl)\n",
    "#P2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the labels as numbers\n",
    "df['if_pizza'] = df.apply(lambda row: 0 if row['requester_received_pizza'] == False else 1, axis=1)\n",
    "df['if_pizza'].value_counts()\n",
    "\n",
    "# for simple random split, you can and should use cross validation/grid search for later model selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X = df[['request_text', 'request_title']]\n",
    "y = df['requester_received_pizza']\n",
    "\n",
    "# a random pipeline\n",
    "#defines all the steps\n",
    "#we can add a preprocessing componenet if we wish\n",
    "#add feature union for final project \n",
    "#see http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html\n",
    "#label encoder when you have lots of classes: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "  #merge text and numbers for models: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html\n",
    "clf = Pipeline([\n",
    "    ('counter', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())])\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3030, 2)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-afe685474585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 839\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedShuffleSplit(n_splits=10, random_state=1, test_size=0.25,\n",
       "            train_size=None)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseShuffleSplit.get_n_splits of StratifiedShuffleSplit(n_splits=10, random_state=1, test_size=0.25,\n",
       "            train_size=None)>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss.get_n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86      1523\n",
      "          1       0.20      0.00      0.00       497\n",
      "\n",
      "avg / total       0.62      0.75      0.65      2020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 = Pipeline([\n",
    "    ('counter', CountVectorizer(analyzer='word', ngram_range=(1,3))),\n",
    "    ('mnb', MultinomialNB())])\n",
    "\n",
    "sss1 = StratifiedShuffleSplit(test_size=0.5, random_state=0)\n",
    "for train_index, test_index in sss1.split(X, y):\n",
    "    X_train1, X_test1 = X[train_index], X[test_index]\n",
    "    y_train1, y_test1 = y[train_index], y[test_index]\n",
    "\n",
    "clf1.fit(X_train1, y_train1)\n",
    "y_pred1 = clf1.predict(X_test1)\n",
    "print(classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_pred1, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2015,    5])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###look at cross validation\n",
    "##http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86      1523\n",
      "          1       0.00      0.00      0.00       497\n",
      "\n",
      "avg / total       0.57      0.75      0.65      2020\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = re.sub(r'[^a-zA-Z_0-9_\\s]',\" \", x).lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()\n",
    "\n",
    "clf2 = Pipeline([\n",
    "    ('counter', TfidfVectorizer(preprocessor=preprocess, stop_words='english', norm='l2')),\n",
    "    ('mnb', MultinomialNB())])\n",
    "\n",
    "#sss2 = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss1.split(X, y):\n",
    "    X_train2, X_test2 = X[train_index], X[test_index]\n",
    "    y_train2, y_test2 = y[train_index], y[test_index]\n",
    "\n",
    "clf2.fit(X_train2, y_train2)\n",
    "y_pred2 = clf2.predict(X_test2)\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train2, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2284,  746])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "#or wordclouds https://amueller.github.io/word_cloud/\n",
    "unique, counts = np.unique(y_pred2, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2020])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_test2, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1523,  497])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3046\n",
       "True      994\n",
       "Name: requester_received_pizza, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['requester_received_pizza'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86       762\n",
      "          1       0.00      0.00      0.00       248\n",
      "\n",
      "avg / total       0.57      0.75      0.65      1010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# limit the list of categories to make running this example faster.\\ncategories = ['alt.atheism', 'talk.religion.misc']\\ntrain = fetch_20newsgroups(random_state=1,\\n                           subset='train',\\n                           categories=categories,\\n                           )\\ntest = fetch_20newsgroups(random_state=1,\\n                          subset='test',\\n                          categories=categories,\\n                          )\\n\\npipeline.fit(train.data, train.target)\\ny = pipeline.predict(test.data)\\nprint(classification_report(y, test.target))\\n\""
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('get-request', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_text')),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])),\n",
    "            \n",
    "            \"\"\"('get-number', Pipeline([\n",
    "                ('selector', ItemSelector(key='number'))\n",
    "            ]))\"\"\"\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "'''\n",
    "# limit the list of categories to make running this example faster.\n",
    "categories = ['alt.atheism', 'talk.religion.misc']\n",
    "train = fetch_20newsgroups(random_state=1,\n",
    "                           subset='train',\n",
    "                           categories=categories,\n",
    "                           )\n",
    "test = fetch_20newsgroups(random_state=1,\n",
    "                          subset='test',\n",
    "                          categories=categories,\n",
    "                          )\n",
    "\n",
    "pipeline.fit(train.data, train.target)\n",
    "y = pipeline.predict(test.data)\n",
    "print(classification_report(y, test.target))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do this for step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
