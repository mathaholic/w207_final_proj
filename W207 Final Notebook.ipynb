{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project: Random Acts of Pizza\n",
    "\n",
    "#### By:  David Skarbrevik, Jayashree Ramen, and Nikki Haas\n",
    "#### MIDS W207:  Intro to Machine Learning\n",
    "#### University of California, Berkeley\n",
    "#### Spring, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SettingWithCopyWarning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-2b3cf591dd4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRuntimeWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSettingWithCopyWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;31m#General Libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SettingWithCopyWarning' is not defined"
     ]
    }
   ],
   "source": [
    "#some of the SKLearn features we learnt in class are being depreciated.  They still exist for now, so let's ignore \n",
    "#the watnings\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "#General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from subprocess import check_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "#SkLearn construction items\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning / Manipulating the Data / EDA / Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features in Training Data\n",
      "0 giver_username_if_known\n",
      "1 number_of_downvotes_of_request_at_retrieval\n",
      "2 number_of_upvotes_of_request_at_retrieval\n",
      "3 post_was_edited\n",
      "4 request_id\n",
      "5 request_number_of_comments_at_retrieval\n",
      "6 request_text\n",
      "7 request_text_edit_aware\n",
      "8 request_title\n",
      "9 requester_account_age_in_days_at_request\n",
      "10 requester_account_age_in_days_at_retrieval\n",
      "11 requester_days_since_first_post_on_raop_at_request\n",
      "12 requester_days_since_first_post_on_raop_at_retrieval\n",
      "13 requester_number_of_comments_at_request\n",
      "14 requester_number_of_comments_at_retrieval\n",
      "15 requester_number_of_comments_in_raop_at_request\n",
      "16 requester_number_of_comments_in_raop_at_retrieval\n",
      "17 requester_number_of_posts_at_request\n",
      "18 requester_number_of_posts_at_retrieval\n",
      "19 requester_number_of_posts_on_raop_at_request\n",
      "20 requester_number_of_posts_on_raop_at_retrieval\n",
      "21 requester_number_of_subreddits_at_request\n",
      "22 requester_received_pizza\n",
      "23 requester_subreddits_at_request\n",
      "24 requester_upvotes_minus_downvotes_at_request\n",
      "25 requester_upvotes_minus_downvotes_at_retrieval\n",
      "26 requester_upvotes_plus_downvotes_at_request\n",
      "27 requester_upvotes_plus_downvotes_at_retrieval\n",
      "28 requester_user_flair\n",
      "29 requester_username\n",
      "30 unix_timestamp_of_request\n",
      "31 unix_timestamp_of_request_utc\n",
      "\n",
      "\n",
      "All Features in Test Data (fewer than Training Data)\n",
      "0 giver_username_if_known\n",
      "1 request_id\n",
      "2 request_text_edit_aware\n",
      "3 request_title\n",
      "4 requester_account_age_in_days_at_request\n",
      "5 requester_days_since_first_post_on_raop_at_request\n",
      "6 requester_number_of_comments_at_request\n",
      "7 requester_number_of_comments_in_raop_at_request\n",
      "8 requester_number_of_posts_at_request\n",
      "9 requester_number_of_posts_on_raop_at_request\n",
      "10 requester_number_of_subreddits_at_request\n",
      "11 requester_subreddits_at_request\n",
      "12 requester_upvotes_minus_downvotes_at_request\n",
      "13 requester_upvotes_plus_downvotes_at_request\n",
      "14 requester_username\n",
      "15 unix_timestamp_of_request\n",
      "16 unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "### data needed:\n",
    "#binarized categorical data: df6_data \n",
    "#numeric data: num_data\n",
    "#numeric data cannot be used as is.  \n",
    "#Take num_data and normalize all fields\n",
    "#text data:  data_set\n",
    "#text data's pipeline masks some of the data, let's redo the Tf-Idf vectorization\n",
    "\n",
    "train_data = pd.read_json('train.json', orient='columns')\n",
    "train_labels = train_data['requester_received_pizza']\n",
    "test_data = pd.read_json('test.json', orient='columns')\n",
    "\n",
    "print(\"All Features in Training Data\")\n",
    "for x,y in enumerate(train_data.columns):\n",
    "    print(x,y)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"All Features in Test Data (fewer than Training Data)\")\n",
    "for x,y in enumerate(test_data.columns):\n",
    "    print(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Binarized Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def day_time(x):\n",
    "    y = ''\n",
    "    if datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 10:\n",
    "        y = 'morning'\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 10 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 16:\n",
    "        y = 'midday'\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 16 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 21:\n",
    "        y = 'evening'\n",
    "    else: \n",
    "        y = 'late_night'\n",
    "    return y\n",
    "\n",
    "day_values = ['morning', 'midday', 'evening', 'late_night']\n",
    "    \n",
    "def human_time(a):\n",
    "    import datetime\n",
    "    from datetime import date\n",
    "    import calendar\n",
    "    ### for the data in raop, return human time.  maybe the time of day matters\n",
    "    a['human_readable_local_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['human_readable_UTC_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['weekday'] = calendar.day_name[datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).weekday()].lower()\n",
    "    a['month'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime(\"%B\").lower()\n",
    "    a['time_of_day'] = day_time(a)\n",
    "    return a\n",
    "\n",
    "a = pd.read_json('train.json')\n",
    "b = a.apply(human_time, axis=1)\n",
    "c = pd.read_json('test.json')\n",
    "d = c.apply(human_time, axis =1) \n",
    "\n",
    "#parse the training set to match the testing set's columns\n",
    "df = b[list(d.columns)]\n",
    "tstdf = d[list(d.columns)]\n",
    "\n",
    "b = pd.get_dummies(df['weekday'])\n",
    "tstb = pd.get_dummies(tstdf['weekday'])\n",
    "c = pd.get_dummies(df['time_of_day'])\n",
    "tstc = pd.get_dummies(tstdf['time_of_day'])\n",
    "d= pd.get_dummies(df['month'])\n",
    "tstd= pd.get_dummies(tstdf['month'])\n",
    "df2 = pd.concat([df,b,c,d], axis=1)\n",
    "tstdf2 = pd.concat([tstdf,tstb,tstc,tstd], axis=1)\n",
    "\n",
    "del df2['month']\n",
    "del tstdf2['month']\n",
    "del df2['weekday']\n",
    "del tstdf2['weekday']\n",
    "del df2['time_of_day']\n",
    "del tstdf2['time_of_day']\n",
    "\n",
    "df4 = pd.get_dummies(df2['requester_subreddits_at_request'].apply(pd.Series), prefix='', prefix_sep='').sum(level=0, axis=1)\n",
    "tstdf4 = pd.get_dummies(tstdf2['requester_subreddits_at_request'].apply(pd.Series), prefix='', prefix_sep='').sum(level=0, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df2 = df2[[ 'request_id','friday',\n",
    "       'monday', 'saturday', 'sunday', 'thursday', 'tuesday', 'wednesday',\n",
    "       'evening', 'late_night', 'midday', 'morning', 'april', 'august',\n",
    "       'december', 'february', 'january', 'july', 'june', 'march', 'may',\n",
    "       'november', 'october', 'september']]\n",
    "tstdf2 = tstdf2[[ 'request_id','friday',\n",
    "       'monday', 'saturday', 'sunday', 'thursday', 'tuesday', 'wednesday',\n",
    "       'evening', 'late_night', 'midday', 'morning', 'april', 'august',\n",
    "       'december', 'february', 'january', 'july', 'june', 'march', 'may',\n",
    "       'november', 'october', 'september']]\n",
    "\n",
    "df5 = pd.concat([a['requester_received_pizza'],df2,df4], axis = 1)\n",
    "tstdf5 = pd.concat([tstdf2,tstdf4], axis = 1)\n",
    "\n",
    "del df5['request_id']\n",
    "del tstdf5['request_id']\n",
    "\n",
    "df6_labels = df5['requester_received_pizza']\n",
    "del df5['requester_received_pizza']\n",
    "df6_data = df5\n",
    "\n",
    "bincat_train_data = df6_data[0:2019]\n",
    "bincat_train_labels = df6_labels[0:2019]\n",
    "bincat_test_data = df6_data[2020:]\n",
    "bincat_test_labels = df6_labels[2020:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten vote counts for posts: \n",
      " [1286864, 789287, 237572, 194891, 184831, 149645, 139199, 110450, 107283, 106518]\n",
      "\n",
      " Are there any missing values?: False\n"
     ]
    }
   ],
   "source": [
    "# some posts have crazy high vote counts, should consider normalization\n",
    "print(\"Top ten vote counts for posts: \\n\", list(np.sort(train_data.iloc[:,26])[:-11:-1]))\n",
    "\n",
    "# indices of all numeric variables (can't use this in models, b/c not all features will be in final test data)\n",
    "all_count_num_vars = [1,2,5,9,10,11,12,13,14,15,16,17,18,19,20,21,24,25,26,27]\n",
    "\n",
    "#numeric data that is only is in test data (excludes numerics that are only in train data)\n",
    "cols = ['requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request','requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request']\n",
    "\n",
    "# subsetting all numeric variables in test data\n",
    "#num_data = train_data.iloc[:,all_count_num_vars]\n",
    "num_data = train_data[cols]\n",
    "\n",
    "\n",
    "# are there any NaNs?\n",
    "print(\"\\n Are there any missing values?:\", num_data.isnull().values.any())\n",
    "\n",
    "\n",
    "# normalization of numeric data for later models\n",
    "scl = MaxAbsScaler()\n",
    "num_norm = pd.DataFrame(index=num_data.index)\n",
    "\n",
    "for col in cols:\n",
    "    scl.fit(num_data[col].values.reshape(-1,1))\n",
    "    c = col+'_scaled'\n",
    "    num_norm[c] = scl.transform(num_data[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC-1           pics\n",
      "PC-2         midday\n",
      "PC-3     reddit.com\n",
      "PC-4           july\n",
      "PC-5      worldnews\n",
      "PC-6        morning\n",
      "PC-7          trees\n",
      "PC-8         gaming\n",
      "PC-9          trees\n",
      "PC-10          IAmA\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit_transform(bincat_train_data)\n",
    "ind = ['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9','PC-10']\n",
    "\n",
    "# Dump components relations with features:\n",
    "\n",
    "pca_df = pd.DataFrame(pca.components_,columns=bincat_train_data.columns,index = ind)\n",
    "\n",
    "\n",
    "\n",
    "best_cat_cols = pca_df.idxmax(axis=1)\n",
    "\n",
    "print(best_cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Looking at Numeric Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Model \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.99      0.86      1532\n",
      "       True       0.63      0.03      0.07       488\n",
      "\n",
      "avg / total       0.73      0.76      0.67      2020\n",
      "\n",
      "Accuracy Score for this model =  0.761881188119\n",
      "\n",
      "\n",
      "\n",
      "Results of Random Forests Implementation \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.92      0.84      1532\n",
      "       True       0.35      0.14      0.20       488\n",
      "\n",
      "avg / total       0.67      0.73      0.68      2020\n",
      "\n",
      "Accuracy Score for this model =  0.730693069307\n",
      "\n",
      "\n",
      "\n",
      "Results of XGBoost Implementation \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.98      0.86      1532\n",
      "       True       0.48      0.06      0.11       488\n",
      "\n",
      "avg / total       0.70      0.76      0.68      2020\n",
      "\n",
      "Accuracy Score for this model =  0.757425742574\n"
     ]
    }
   ],
   "source": [
    "# randomizing the data (if we want to do this)\n",
    "#shuffle = np.random.permutation(np.arange(num_data.shape[0]))\n",
    "#num_train_data, num_train_labels = num_data[shuffle], num_labels[shuffle]\n",
    "\n",
    "# split for training/development\n",
    "num_train_data = num_data[0:2019]\n",
    "num_train_labels = train_labels[0:2019]\n",
    "num_test_data = num_data[2020:]\n",
    "num_test_labels = train_labels[2020:]\n",
    "\n",
    "\n",
    "# simple logistic regression model\n",
    "lf = LogisticRegression(C = 100)\n",
    "lf.fit(num_train_data, num_train_labels)\n",
    "\n",
    "preds = lf.predict(num_test_data)\n",
    "\n",
    "print(\"Results of Logistic Regression Model \\n\", classification_report(num_test_labels,preds))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# simple Random Forests\n",
    "lf2 = RandomForestClassifier(n_estimators=12)\n",
    "lf2.fit(num_train_data, num_train_labels)\n",
    "\n",
    "preds2 = lf2.predict(num_test_data)\n",
    "\n",
    "print(\"Results of Random Forest Implementation \\n\", classification_report(num_test_labels,preds2))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds2))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# trying a boosting algorithm\n",
    "gbm = xgb.XGBClassifier()\n",
    "gbm.fit(num_train_data, num_train_labels)\n",
    "preds3 = gbm.predict(num_test_data)\n",
    "\n",
    "print(\"Results of XGBoost Implementation \\n\", classification_report(num_test_labels,preds3))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Random Forest Implementation \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.95      0.84      1532\n",
      "       True       0.28      0.06      0.10       488\n",
      "\n",
      "avg / total       0.64      0.73      0.67      2020\n",
      "\n",
      "Accuracy Score for this model =  0.734158415842\n"
     ]
    }
   ],
   "source": [
    "# trying RandomForest\n",
    "clf2 = RandomForestClassifier(n_estimators=12)\n",
    "clf2.fit(bincat_train_data, bincat_train_labels)\n",
    "\n",
    "preds2 = clf2.predict(bincat_test_data)\n",
    "\n",
    "print(\"Results of Random Forest Implementation \\n\", classification_report(num_test_labels,preds2))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is very similar to the numerical data by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFgCAYAAABqjwo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e8b39f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression on Text Data \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.76      0.81      0.78       762\n",
      "  Got pizza       0.27      0.22      0.24       248\n",
      "\n",
      "avg / total       0.64      0.66      0.65      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Pull in text data\n",
    "a = pd.read_json('train.json', orient='columns')\n",
    "data_set = a[['request_text_edit_aware','request_title']]\n",
    "text_data = pd.concat([df6_labels, data_set], axis = 1) \n",
    "data_labels = a['requester_received_pizza']\n",
    "\n",
    "###########################################################################################\n",
    "## Considering the effect of the length of a post\n",
    "\n",
    "print('Average Length of Granted Request:', np.average(text_data[text_data['requester_received_pizza'] == True]['request_length']),\n",
    "      '\\nAverage Length of Failed Request:', np.average(text_data[text_data['requester_received_pizza'] == False]['request_length']))\n",
    "\n",
    "bunny = text_data[text_data['requester_received_pizza'] == True]\n",
    "bunny['request_length'] = bunny['request_text_edit_aware'].map(lambda x: len(x.split()))\n",
    "np.average(bunny['request_length'])\n",
    "\n",
    "jackal = text_data[text_data['requester_received_pizza'] == False]\n",
    "jackal['request_length'] = jackal['request_text_edit_aware'].map(lambda x: len(x.split()))\n",
    "np.average(jackal['request_length'])\n",
    "\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "#x = pd.Series(bunny['request_length'], name = 'request granted lenth')\n",
    "#ax = sns.distplot(x)\n",
    "sns.set(rc={\"figure.figsize\": (8, 5)});\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(bunny['request_length'], ax=ax, color='green',kde=False, rug=False,bins=200)\n",
    "sns.distplot(jackal['request_length'], ax=ax, color='red',kde=False, rug=False,bins=200)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Distributions of Request Lengths, By Success Status')\n",
    "plt.legend(loc='upper left')\n",
    "sns.plt.show()\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = re.sub(r'[^a-zA-Z_0-9_\\s]',\" \", x).lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "# Feature Union for post title and post text, w/ logistic regression\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "                ('cv', TfidfVectorizer(analyzer='char_wb', \n",
    "                                       #vocabulary=vocabulary,\n",
    "                                       preprocessor=preprocess,\n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('get-request', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "                ('cv', TfidfVectorizer(analyzer='char_wb', \n",
    "                                       #vocabulary=vocabulary,\n",
    "                                       preprocessor=preprocess,\n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "            \n",
    "           #('get-number', Pipeline([\n",
    "           #     ('selector', ItemSelector(key='number'))\n",
    "           # ]))\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('nb',LogisticRegression(penalty='l2',C=100)),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "text_pred = pipeline.predict(X_test)\n",
    "target_names = [ 'No pizza','Got pizza']\n",
    "print(\"Results of Logistic Regression on Text Data \\n\", classification_report(y_test, text_pred, target_names=target_names))\n",
    "\n",
    "top_word_pairs = get_top_words(text_data['requester_received_pizza'], tkn2_request, fit2_req)\n",
    "#print(\"\\nPaired Word Frequency Matrix: Top Words per Article Type \\n\\t\",', '.join(newsgroups_train.target_names))\n",
    "top_term_pairs = get_top_terms(top_word_pairs[0], top_word_pairs[1], fit2_req, text_data['requester_received_pizza'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning for Ada Boost\n",
    "\n",
    "The module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995].\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights w_1, w_2, ..., w_N to each of the training samples. Initially, those weights are all set to w_i = 1/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence [HTF]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.78      0.91      0.84       609\n",
      "  Got pizza       0.46      0.23      0.31       199\n",
      "\n",
      "avg / total       0.71      0.75      0.71       808\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_norm_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-605431b65e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfeature_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntstdf6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         csr_matrix(num_norm_test)), \n\u001b[0m\u001b[1;32m     65\u001b[0m         format='csr')\n\u001b[1;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_norm_test' is not defined"
     ]
    }
   ],
   "source": [
    "#best kaggle score\n",
    "### add a stemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "### change the binarized test data to contain the same columns as the train data\n",
    "#df6_cols = [x for x in df6_data.columns if x in tstdf6.columns]\n",
    "ntstdf6 = pd.DataFrame(index=tstdf5.index)\n",
    "for x in list(df6_data.columns):\n",
    "    if x in tstdf5.columns:\n",
    "        ntstdf6[x] = tstdf5[x]\n",
    "    else:\n",
    "        ntstdf6[x] = 0\n",
    "ntstdf6.shape\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "tkn_request = TfidfVectorizer(preprocessor=preprocess,\n",
    "                              stop_words='english',\n",
    "                              #vocabulary=good_words,\n",
    "                             analyzer = stemmed_words)\n",
    "tkn_title = TfidfVectorizer(preprocessor=preprocess,\n",
    "                            stop_words='english',\n",
    "                           analyzer = stemmed_words)\n",
    "feature_request = tkn_request.fit_transform(text_data['request_text_edit_aware'])\n",
    "feature_title = tkn_title.fit_transform(text_data['request_title'])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "X = hstack((\n",
    "        csr_matrix(feature_request),\n",
    "        csr_matrix(feature_title),\n",
    "        csr_matrix(df6_data), \n",
    "        csr_matrix(num_norm)), \n",
    "        format='csr')\n",
    "\n",
    "y = text_data['requester_received_pizza']\n",
    "\n",
    "# shuffle data\n",
    "sss = StratifiedShuffleSplit(test_size=\n",
    "                             0.20, random_state=1)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "# implement ada boost classifier\n",
    "ad2 = AdaBoostClassifier()\n",
    "ad2.fit(X_train, y_train)\n",
    "ad2_pred = ad2.predict(X_test)\n",
    "print(\"Results for AdaBoost Implementation: \\n\", classification_report(y_test, ad2_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "tfeature_request = tkn_request.transform(tstdf['request_text_edit_aware']) \n",
    "tfeature_title = tkn_title.transform(tstdf['request_title'])\n",
    "\n",
    "Xt = hstack((\n",
    "        csr_matrix(tfeature_request),\n",
    "        csr_matrix(tfeature_title),\n",
    "        csr_matrix(ntstdf6), \n",
    "        csr_matrix(num_norm_test)), \n",
    "        format='csr')\n",
    "\n",
    "# get results\n",
    "Y_pred = ad2.predict(Xt)\n",
    "\n",
    "# consider if number of answered requests for pizza is similiar to train data\n",
    "np.average(Y_pred)\n",
    "\n",
    "# put in proper submission format\n",
    "Y_pred_2 = vint(Y_pred)\n",
    "fin_df = pd.concat([test['request_id'],pd.DataFrame(Y_pred_2, columns=['requester_received_pizza'])],axis =1)\n",
    "fin_df['requester_received_pizza'] = pd.to_numeric(fin_df['requester_received_pizza'],downcast='signed')\n",
    "\n",
    "# verify format visually\n",
    "fin_df.head()\n",
    "\n",
    "# output to file\n",
    "fin_df.to_csv('pizza_submission11.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Appendix\n",
    "\n",
    "Here you will miscellaneous techniques performed that did not necessarily inform the decision of the final model. This section may not be as organized as the rest of the notebook, but is included for posterity and possible usefulness in any future analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
