{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/nicholeh/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "#make the depreciation warnings go away\n",
    "import warnings\n",
    "#I'm tired of the warnings on functions the professor asks us to use :) \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "print(sys.version)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 giver_username_if_known\n",
      "1 number_of_downvotes_of_request_at_retrieval\n",
      "2 number_of_upvotes_of_request_at_retrieval\n",
      "3 post_was_edited\n",
      "4 request_id\n",
      "5 request_number_of_comments_at_retrieval\n",
      "6 request_text\n",
      "7 request_text_edit_aware\n",
      "8 request_title\n",
      "9 requester_account_age_in_days_at_request\n",
      "10 requester_account_age_in_days_at_retrieval\n",
      "11 requester_days_since_first_post_on_raop_at_request\n",
      "12 requester_days_since_first_post_on_raop_at_retrieval\n",
      "13 requester_number_of_comments_at_request\n",
      "14 requester_number_of_comments_at_retrieval\n",
      "15 requester_number_of_comments_in_raop_at_request\n",
      "16 requester_number_of_comments_in_raop_at_retrieval\n",
      "17 requester_number_of_posts_at_request\n",
      "18 requester_number_of_posts_at_retrieval\n",
      "19 requester_number_of_posts_on_raop_at_request\n",
      "20 requester_number_of_posts_on_raop_at_retrieval\n",
      "21 requester_number_of_subreddits_at_request\n",
      "22 requester_received_pizza\n",
      "23 requester_subreddits_at_request\n",
      "24 requester_upvotes_minus_downvotes_at_request\n",
      "25 requester_upvotes_minus_downvotes_at_retrieval\n",
      "26 requester_upvotes_plus_downvotes_at_request\n",
      "27 requester_upvotes_plus_downvotes_at_retrieval\n",
      "28 requester_user_flair\n",
      "29 requester_username\n",
      "30 unix_timestamp_of_request\n",
      "31 unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_json('train.json')\n",
    "for x, y in enumerate(a.columns):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nikki's first three hypotheses: date/time will matter\n",
    "\n",
    "import datetime\n",
    "def day_time(x):\n",
    "    y = ''\n",
    "    if datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 10:\n",
    "        y = 0\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 10 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 16:\n",
    "        y = 1\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 16 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 21:\n",
    "        y = 2\n",
    "    else: \n",
    "        y = 3\n",
    "    return y\n",
    "\n",
    "day_values = ['morning', 'midday', 'evening', 'latenight']\n",
    "    \n",
    "def human_time(a):\n",
    "    import datetime\n",
    "    from datetime import date\n",
    "    import calendar\n",
    "    ### for the data in raop, return human time.  maybe the time of day matters\n",
    "    a['human_readable_local_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['human_readable_UTC_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['weekday'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).weekday()\n",
    "    a['month'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).month\n",
    "    a['time_of_day'] = day_time(a)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.apply(human_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### add in number of downvotes.  The test data doesn't have this, it only has these aggregate values.  Derive number\n",
    "### of downvotes\n",
    "### we're given \n",
    "###up + down = col_26 \n",
    "###and \n",
    "### up-down = col_24\n",
    "### solve algebraically,\n",
    "# 2up = col_26+col_24,\n",
    "# up = (col_26+col_24)/2\n",
    "# (col_26+col_24)/2 + down = col_26\n",
    "#down = col_26 - (col_26+col_24)/2\n",
    "#down = (col_26 - col_24)/2\n",
    "\n",
    "b['total_downvotes'] = (b['requester_upvotes_plus_downvotes_at_request'] - b['requester_upvotes_minus_downvotes_at_request'])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = re.sub(r'[^a-zA-Z_0-9_\\s]',\" \", x).lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "#try the Pipeline implementation from sk learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class WeekdayExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        shape = data_dict.shape\n",
    "        return data_dict[self.key].reshape(shape[0],1)\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "class NBWrapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, NB):\n",
    "        self.NB = NB\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self.NB.fit(x, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return self.NB.predict(posts)\n",
    "\n",
    "pipeline4= Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ### add in text variables\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "           #     ('cv', CountVectorizer(analyzer='char_wb', \n",
    "           #                           #vocabulary=vocabulary,\n",
    "           #                            #preprocessor=preprocess,\n",
    "           #                          max_df=0.5, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfVectorizer(min_df=65, \n",
    "                                          preprocessor=preprocess\n",
    "                                         ))\n",
    "            ])),\n",
    "\n",
    "            \n",
    "            #('get-text', Pipeline([\n",
    "            #    ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "            #    ('tfidf', TfidfVectorizer(min_df=50))\n",
    "                #('cv', CountVectorizer(analyzer='char_wb', \n",
    "                #                       #vocabulary=vocabulary,\n",
    "                #                       max_df=0.5, ngram_range=(1,3)))\n",
    "            #])),\n",
    "         ### adding in categorical variables\n",
    "           # research different encoders\n",
    "           ('get-time_of_day', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='time_of_day')),\n",
    "                ('one_hot', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "        \n",
    "           ('get-weekday', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='weekday')),\n",
    "                ('lh', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "          ('get-month', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='month')),\n",
    "                ('2h', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "                    \n",
    "        ### add numeric data\n",
    "        #research better numeric extractors and coders\n",
    "         ('get-requester_number_of_subreddits_at_request', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_number_of_subreddits_at_request')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "            \n",
    "         ('get-downvotes', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='total_downvotes')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))   \n",
    "                \n",
    "           ]))\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('estimator',LogisticRegression(penalty='l2',C=25))\n",
    "    #('estimator', SVC(kernel='linear')) \n",
    "    \n",
    "    ('estimator', DecisionTreeClassifier())\n",
    "    #('estimator', RandomForestClassifier())\n",
    "    #('estimator', MultinomialNB())\n",
    "    #('estmator', KNeighborsClassifier())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.78      0.78      0.78       762\n",
      "  Got Pizza       0.31      0.31      0.31       248\n",
      "\n",
      "avg / total       0.66      0.66      0.66      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = b[['request_title','requester_number_of_subreddits_at_request', \n",
    "              'time_of_day','weekday', 'month', 'total_downvotes']]\n",
    "#data_set_title = a['request_title']\n",
    "data_labels = b['requester_received_pizza']\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]\n",
    "\n",
    "pipeline4.fit(X_train, y_train)\n",
    "y_pred = pipeline4.predict(X_test)\n",
    "target_names = ['No pizza','Got Pizza']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[594, 172],\n",
       "       [168,  76]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_json('test.json')\n",
    "test['total_downvotes'] = (test['requester_upvotes_plus_downvotes_at_request'] - test['requester_upvotes_minus_downvotes_at_request'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1 = test.apply(human_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>requester_days_since_first_post_on_raop_at_request</th>\n",
       "      <th>requester_number_of_comments_at_request</th>\n",
       "      <th>requester_number_of_comments_in_raop_at_request</th>\n",
       "      <th>requester_number_of_posts_at_request</th>\n",
       "      <th>requester_number_of_posts_on_raop_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_upvotes_plus_downvotes_at_request</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "      <th>total_downvotes</th>\n",
       "      <th>human_readable_local_time</th>\n",
       "      <th>human_readable_UTC_time</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>time_of_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>42.083866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>840</td>\n",
       "      <td>j_like</td>\n",
       "      <td>1308963419</td>\n",
       "      <td>1308959819</td>\n",
       "      <td>238.0</td>\n",
       "      <td>2011-06-24 17:56:59</td>\n",
       "      <td>2011-06-24 16:56:59</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>223.784537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1448</td>\n",
       "      <td>0110110101101100</td>\n",
       "      <td>1379263523</td>\n",
       "      <td>1379259923</td>\n",
       "      <td>466.0</td>\n",
       "      <td>2013-09-15 09:45:23</td>\n",
       "      <td>2013-09-15 08:45:23</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>singledad22601</td>\n",
       "      <td>1318636421</td>\n",
       "      <td>1318632821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-14 16:53:41</td>\n",
       "      <td>2011-10-14 15:53:41</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>481.311273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2062</td>\n",
       "      <td>Neuronut</td>\n",
       "      <td>1375220282</td>\n",
       "      <td>1375216682</td>\n",
       "      <td>502.0</td>\n",
       "      <td>2013-07-30 14:38:02</td>\n",
       "      <td>2013-07-30 13:38:02</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>so_damn_hungry</td>\n",
       "      <td>1335934358</td>\n",
       "      <td>1335930758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-05-01 21:52:38</td>\n",
       "      <td>2012-05-01 20:52:38</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known request_id  \\\n",
       "0                     N/A   t3_i8iy4   \n",
       "1                     N/A  t3_1mfqi0   \n",
       "2                     N/A   t3_lclka   \n",
       "3                     N/A  t3_1jdgdj   \n",
       "4                     N/A   t3_t2qt4   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hey all! It's about 95 degrees here and our ki...   \n",
       "1  I didn't know a place like this exists! \\n\\nI ...   \n",
       "2  Hi Reddit. Im a single dad having a really rou...   \n",
       "3  Hi I just moved to Waltham MA from my home sta...   \n",
       "4  We're just sitting here near indianapolis on o...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0  [request] pregger gf 95 degree house and no fo...   \n",
       "1  [Request] Lost my job day after labour day, st...   \n",
       "2                (Request) pizza for my kids please?   \n",
       "3  [Request] Just moved to a new state(Waltham MA...   \n",
       "4  [Request] Two girls in between paychecks, we'v...   \n",
       "\n",
       "   requester_account_age_in_days_at_request  \\\n",
       "0                                 42.083866   \n",
       "1                                223.784537   \n",
       "2                                  0.000000   \n",
       "3                                481.311273   \n",
       "4                                  0.000000   \n",
       "\n",
       "   requester_days_since_first_post_on_raop_at_request  \\\n",
       "0                                                0.0    \n",
       "1                                                0.0    \n",
       "2                                                0.0    \n",
       "3                                                0.0    \n",
       "4                                                0.0    \n",
       "\n",
       "   requester_number_of_comments_at_request  \\\n",
       "0                                       57   \n",
       "1                                      145   \n",
       "2                                        0   \n",
       "3                                      277   \n",
       "4                                        0   \n",
       "\n",
       "   requester_number_of_comments_in_raop_at_request  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   requester_number_of_posts_at_request  \\\n",
       "0                                    10   \n",
       "1                                    36   \n",
       "2                                     0   \n",
       "3                                    17   \n",
       "4                                     0   \n",
       "\n",
       "   requester_number_of_posts_on_raop_at_request     ...      \\\n",
       "0                                             0     ...       \n",
       "1                                             0     ...       \n",
       "2                                             0     ...       \n",
       "3                                             0     ...       \n",
       "4                                             0     ...       \n",
       "\n",
       "   requester_upvotes_plus_downvotes_at_request requester_username  \\\n",
       "0                                          840             j_like   \n",
       "1                                         1448   0110110101101100   \n",
       "2                                            0     singledad22601   \n",
       "3                                         2062           Neuronut   \n",
       "4                                            0     so_damn_hungry   \n",
       "\n",
       "   unix_timestamp_of_request  unix_timestamp_of_request_utc total_downvotes  \\\n",
       "0                 1308963419                     1308959819           238.0   \n",
       "1                 1379263523                     1379259923           466.0   \n",
       "2                 1318636421                     1318632821             0.0   \n",
       "3                 1375220282                     1375216682           502.0   \n",
       "4                 1335934358                     1335930758             0.0   \n",
       "\n",
       "   human_readable_local_time  human_readable_UTC_time  weekday month  \\\n",
       "0        2011-06-24 17:56:59      2011-06-24 16:56:59        4     6   \n",
       "1        2013-09-15 09:45:23      2013-09-15 08:45:23        6     9   \n",
       "2        2011-10-14 16:53:41      2011-10-14 15:53:41        4    10   \n",
       "3        2013-07-30 14:38:02      2013-07-30 13:38:02        1     7   \n",
       "4        2012-05-01 21:52:38      2012-05-01 20:52:38        1     5   \n",
       "\n",
       "  time_of_day  \n",
       "0           2  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           2  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = test1[['request_title','requester_number_of_subreddits_at_request', \n",
    "              'time_of_day','weekday', 'month', 'total_downvotes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_number_of_subreddits_at_request</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>total_downvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>502.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       request_title  \\\n",
       "0  [request] pregger gf 95 degree house and no fo...   \n",
       "1  [Request] Lost my job day after labour day, st...   \n",
       "2                (Request) pizza for my kids please?   \n",
       "3  [Request] Just moved to a new state(Waltham MA...   \n",
       "4  [Request] Two girls in between paychecks, we'v...   \n",
       "\n",
       "   requester_number_of_subreddits_at_request  time_of_day  weekday  month  \\\n",
       "0                                         16            2        4      6   \n",
       "1                                         29            0        6      9   \n",
       "2                                          0            1        4     10   \n",
       "3                                         30            1        1      7   \n",
       "4                                          0            2        1      5   \n",
       "\n",
       "   total_downvotes  \n",
       "0            238.0  \n",
       "1            466.0  \n",
       "2              0.0  \n",
       "3            502.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = pipeline4.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25015328019619865"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id  requester_received_pizza\n",
       "0   t3_i8iy4                         1\n",
       "1  t3_1mfqi0                         0\n",
       "2   t3_lclka                         0\n",
       "3  t3_1jdgdj                         0\n",
       "4   t3_t2qt4                         1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inter(x):\n",
    "    return int(x)\n",
    "vint = np.vectorize(inter)\n",
    "test_pred2 = vint(test_pred)\n",
    "fin_df = pd.concat([test1['request_id'],pd.DataFrame(test_pred2, columns=['requester_received_pizza'])],axis =1)\n",
    "fin_df['requester_received_pizza'] = pd.to_numeric(fin_df['requester_received_pizza'],downcast='signed')\n",
    "fin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_df.to_csv('pizza_submission13.csv', sep = ',', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "#try the Pipeline implementation from sk learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class WeekdayExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        shape = data_dict.shape\n",
    "        return data_dict[self.key].reshape(shape[0],1)\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "class NBWrapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, NB):\n",
    "        self.NB = NB\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self.NB.fit(x, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return self.NB.predict(posts)\n",
    "\n",
    "pipeline4= Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ### add in text variables\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "           #     ('cv', CountVectorizer(analyzer='char_wb', \n",
    "           #                           #vocabulary=vocabulary,\n",
    "           #                            #preprocessor=preprocess,\n",
    "           #                          max_df=0.5, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfVectorizer(min_df=65, \n",
    "                                          preprocessor=preprocess\n",
    "                                         ))\n",
    "            ])),\n",
    "\n",
    "            \n",
    "            #('get-text', Pipeline([\n",
    "            #    ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "            #    ('tfidf', TfidfVectorizer(min_df=50))\n",
    "                #('cv', CountVectorizer(analyzer='char_wb', \n",
    "                #                       #vocabulary=vocabulary,\n",
    "                #                       max_df=0.5, ngram_range=(1,3)))\n",
    "            #])),\n",
    "         ### adding in categorical variables\n",
    "           # research different encoders\n",
    "           ('get-time_of_day', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='time_of_day')),\n",
    "                ('one_hot', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "        \n",
    "           ('get-weekday', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='weekday')),\n",
    "                ('lh', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "          ('get-month', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='month')),\n",
    "                ('2h', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "                    \n",
    "        ### add numeric data\n",
    "        #research better numeric extractors and coders\n",
    "         ('get-requester_number_of_subreddits_at_request', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_number_of_subreddits_at_request')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "            \n",
    "         ('get-downvotes', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='total_downvotes')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))   \n",
    "                \n",
    "           ]))\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('estimator',LogisticRegression(penalty='l2',C=25))\n",
    "    #('estimator', SVC(kernel='linear')) \n",
    "    \n",
    "    ('estimator', AdaBoostClassifier(RandomForestClassifier()))\n",
    "    #('estimator', RandomForestClassifier())\n",
    "    #('estimator', MultinomialNB())\n",
    "    #('estmator', KNeighborsClassifier())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.76      0.95      0.84       762\n",
      "  Got Pizza       0.30      0.07      0.11       248\n",
      "\n",
      "avg / total       0.64      0.73      0.66      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = b[['request_title','requester_number_of_subreddits_at_request', \n",
    "              'time_of_day','weekday', 'month', 'total_downvotes']]\n",
    "\n",
    "\n",
    "\n",
    "#data_set_title = a['request_title']\n",
    "data_labels = b['requester_received_pizza']\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]\n",
    "\n",
    "pipeline4.fit(X_train, y_train)\n",
    "y_pred = pipeline4.predict(X_test)\n",
    "target_names = ['No pizza','Got Pizza']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "b['request_length'] = b['request_text_edit_aware'].map(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "cols = ['requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request','requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request',\n",
    "       'request_length',\n",
    "       'total_downvotes'\n",
    "       ]\n",
    "scl = MaxAbsScaler()\n",
    "num_norm = pd.DataFrame(index=b.index)\n",
    "for col in cols:\n",
    "    scl.fit(b[col].values.reshape(-1,1))\n",
    "    c = col+'_scaled'\n",
    "    num_norm[c] = scl.transform(b[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "#try the Pipeline implementation from sk learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class WeekdayExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        shape = data_dict.shape\n",
    "        return data_dict[self.key].reshape(shape[0],1)\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "class NBWrapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, NB):\n",
    "        self.NB = NB\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self.NB.fit(x, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return self.NB.predict(posts)\n",
    "\n",
    "pipeline4= Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ### add in text variables\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "           #     ('cv', CountVectorizer(analyzer='char_wb', \n",
    "           #                           #vocabulary=vocabulary,\n",
    "           #                            #preprocessor=preprocess,\n",
    "           #                          max_df=0.5, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfVectorizer(min_df=65, \n",
    "                                          preprocessor=preprocess\n",
    "                                         ))\n",
    "            ])),\n",
    "\n",
    "            \n",
    "            #('get-text', Pipeline([\n",
    "            #    ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "            #    ('tfidf', TfidfVectorizer(min_df=50))\n",
    "                #('cv', CountVectorizer(analyzer='char_wb', \n",
    "                #                       #vocabulary=vocabulary,\n",
    "                #                       max_df=0.5, ngram_range=(1,3)))\n",
    "            #])),\n",
    "         ### adding in categorical variables\n",
    "           # research different encoders\n",
    "           ('get-time_of_day', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='time_of_day')),\n",
    "                ('one_hot', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "        \n",
    "           ('get-weekday', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='weekday')),\n",
    "                ('lh', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "          ('get-month', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='month')),\n",
    "                ('2h', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "            ('get-requester_number_of_subreddits_at_request', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_number_of_subreddits_at_request_scaled')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),        \n",
    "        ### add numeric data\n",
    "        #research better numeric extractors and coders\n",
    "         ('get-requester_account_age_in_days_at_request_scaled', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_account_age_in_days_at_request_scaled')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "          \n",
    "                    \n",
    "        ('get-requester_days_since_first_post_on_raop_at_request_scaled', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_days_since_first_post_on_raop_at_request_scaled')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "        ])),\n",
    "        #('get-requester_number_of_comments_at_request_scaled', Pipeline([\n",
    "        #        ('selector', WeekdayExtractor(key='requester_number_of_comments_at_request_scaled')),\n",
    "        #        #('lb', OneHotEncoder())\n",
    "        #        #('NB', NBWrapper(MultinomialNB()))             \n",
    "        #])),    \n",
    "        #('get-requester_number_of_comments_in_raop_at_request_scaled', Pipeline([\n",
    "        #        ('selector', WeekdayExtractor(key='requester_number_of_comments_in_raop_at_request_scaled')),\n",
    "        #        #('lb', OneHotEncoder())\n",
    "        #        #('NB', NBWrapper(MultinomialNB()))             \n",
    "        #])),\n",
    "        #('get-requester_number_of_posts_at_request_scaled', Pipeline([\n",
    "        #        ('selector', WeekdayExtractor(key='requester_number_of_posts_at_request_scaled')),\n",
    "        #        #('lb', OneHotEncoder())\n",
    "        #        #('NB', NBWrapper(MultinomialNB()))             \n",
    "        #])),\n",
    "        #('get-requester_number_of_posts_on_raop_at_request_scaled', Pipeline([\n",
    "        #        ('selector', WeekdayExtractor(key='requester_number_of_posts_on_raop_at_request_scaled')),\n",
    "        #        #('lb', OneHotEncoder())\n",
    "        #        #('NB', NBWrapper(MultinomialNB()))             \n",
    "        #])), \n",
    "        ('get-requester_number_of_subreddits_at_request_scaled', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_number_of_subreddits_at_request_scaled')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "        ])),                     \n",
    "         ('get-downvotes', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='total_downvotes_scaled')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))   \n",
    "                \n",
    "           ]))\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('estimator',LogisticRegression(penalty='l2',C=25))\n",
    "    #('estimator', SVC(kernel='linear')) \n",
    "    \n",
    "    ('estimator', AdaBoostClassifier(RandomForestClassifier()))\n",
    "    #('estimator', RandomForestClassifier())\n",
    "    #('estimator', MultinomialNB())\n",
    "    #('estmator', KNeighborsClassifier())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.76      0.95      0.84       762\n",
      "  Got Pizza       0.27      0.06      0.09       248\n",
      "\n",
      "avg / total       0.64      0.73      0.66      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = pd.concat([b[['request_title', \n",
    "              'time_of_day','weekday', 'month']],num_norm[['requester_account_age_in_days_at_request_scaled',\n",
    "'requester_days_since_first_post_on_raop_at_request_scaled',\n",
    "#'requester_number_of_comments_in_raop_at_request_scaled',\n",
    "#'requester_number_of_posts_on_raop_at_request_scaled',\n",
    "'requester_number_of_subreddits_at_request_scaled',\n",
    "'request_length_scaled',\n",
    "'total_downvotes_scaled']]], axis = 1)\n",
    "#data_set_title = a['request_title']\n",
    "data_labels = b['requester_received_pizza']\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]\n",
    "\n",
    "pipeline4.fit(X_train, y_train)\n",
    "y_pred = pipeline4.predict(X_test)\n",
    "target_names = ['No pizza','Got Pizza']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['requester_account_age_in_days_at_request_scaled',\n",
       "       'requester_days_since_first_post_on_raop_at_request_scaled',\n",
       "       'requester_number_of_comments_at_request_scaled',\n",
       "       'requester_number_of_comments_in_raop_at_request_scaled',\n",
       "       'requester_number_of_posts_at_request_scaled',\n",
       "       'requester_number_of_posts_on_raop_at_request_scaled',\n",
       "       'requester_number_of_subreddits_at_request_scaled',\n",
       "       'requester_upvotes_minus_downvotes_at_request_scaled',\n",
       "       'requester_upvotes_plus_downvotes_at_request_scaled',\n",
       "       'request_length_scaled', 'total_downvotes_scaled'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_norm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>giver_username_if_known</th>\n",
       "      <th>number_of_downvotes_of_request_at_retrieval</th>\n",
       "      <th>number_of_upvotes_of_request_at_retrieval</th>\n",
       "      <th>post_was_edited</th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_number_of_comments_at_retrieval</th>\n",
       "      <th>request_text</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "      <th>requester_account_age_in_days_at_request</th>\n",
       "      <th>...</th>\n",
       "      <th>requester_username</th>\n",
       "      <th>unix_timestamp_of_request</th>\n",
       "      <th>unix_timestamp_of_request_utc</th>\n",
       "      <th>human_readable_local_time</th>\n",
       "      <th>human_readable_UTC_time</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>total_downvotes</th>\n",
       "      <th>request_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>nickylvst</td>\n",
       "      <td>1317852607</td>\n",
       "      <td>1317849007</td>\n",
       "      <td>2011-10-05 15:10:07</td>\n",
       "      <td>2011-10-05 14:10:07</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>0</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>501.111100</td>\n",
       "      <td>...</td>\n",
       "      <td>fohacidal</td>\n",
       "      <td>1332652424</td>\n",
       "      <td>1332648824</td>\n",
       "      <td>2012-03-24 22:13:44</td>\n",
       "      <td>2012-03-24 21:13:44</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>41.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>jacquibatman7</td>\n",
       "      <td>1319650094</td>\n",
       "      <td>1319646494</td>\n",
       "      <td>2011-10-26 10:28:14</td>\n",
       "      <td>2011-10-26 09:28:14</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>4</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>6.518438</td>\n",
       "      <td>...</td>\n",
       "      <td>4on_the_floor</td>\n",
       "      <td>1322855434</td>\n",
       "      <td>1322855434</td>\n",
       "      <td>2011-12-02 11:50:34</td>\n",
       "      <td>2011-12-02 11:50:34</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>5</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>162.063252</td>\n",
       "      <td>...</td>\n",
       "      <td>Futuredogwalker</td>\n",
       "      <td>1373657691</td>\n",
       "      <td>1373654091</td>\n",
       "      <td>2013-07-12 12:34:51</td>\n",
       "      <td>2013-07-12 11:34:51</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>306.0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  giver_username_if_known  number_of_downvotes_of_request_at_retrieval  \\\n",
       "0                     N/A                                            0   \n",
       "1                     N/A                                            2   \n",
       "2                     N/A                                            0   \n",
       "3                     N/A                                            0   \n",
       "4                     N/A                                            6   \n",
       "\n",
       "   number_of_upvotes_of_request_at_retrieval  post_was_edited request_id  \\\n",
       "0                                          1                0   t3_l25d7   \n",
       "1                                          5                0   t3_rcb83   \n",
       "2                                          3                0   t3_lpu5j   \n",
       "3                                          1                1   t3_mxvj3   \n",
       "4                                          6                0  t3_1i6486   \n",
       "\n",
       "   request_number_of_comments_at_retrieval  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        4   \n",
       "4                                        5   \n",
       "\n",
       "                                        request_text  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "4  hey guys:\\n I love this sub. I think it's grea...   \n",
       "\n",
       "                             request_text_edit_aware  \\\n",
       "0  Hi I am in need of food for my 4 children we a...   \n",
       "1  I spent the last money I had on gas today. Im ...   \n",
       "2  My girlfriend decided it would be a good idea ...   \n",
       "3  It's cold, I'n hungry, and to be completely ho...   \n",
       "4  hey guys:\\n I love this sub. I think it's grea...   \n",
       "\n",
       "                                       request_title  \\\n",
       "0            Request Colorado Springs Help Us Please   \n",
       "1  [Request] California, No cash and I could use ...   \n",
       "2  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "   requester_account_age_in_days_at_request       ...        \\\n",
       "0                                  0.000000       ...         \n",
       "1                                501.111100       ...         \n",
       "2                                  0.000000       ...         \n",
       "3                                  6.518438       ...         \n",
       "4                                162.063252       ...         \n",
       "\n",
       "   requester_username  unix_timestamp_of_request  \\\n",
       "0           nickylvst                 1317852607   \n",
       "1           fohacidal                 1332652424   \n",
       "2       jacquibatman7                 1319650094   \n",
       "3       4on_the_floor                 1322855434   \n",
       "4     Futuredogwalker                 1373657691   \n",
       "\n",
       "   unix_timestamp_of_request_utc  human_readable_local_time  \\\n",
       "0                     1317849007        2011-10-05 15:10:07   \n",
       "1                     1332648824        2012-03-24 22:13:44   \n",
       "2                     1319646494        2011-10-26 10:28:14   \n",
       "3                     1322855434        2011-12-02 11:50:34   \n",
       "4                     1373654091        2013-07-12 12:34:51   \n",
       "\n",
       "   human_readable_UTC_time  weekday  month  time_of_day  total_downvotes  \\\n",
       "0      2011-10-05 14:10:07        2     10            1              0.0   \n",
       "1      2012-03-24 21:13:44        5      3            3             41.0   \n",
       "2      2011-10-26 09:28:14        2     10            0              0.0   \n",
       "3      2011-12-02 11:50:34        4     12            1             11.0   \n",
       "4      2013-07-12 11:34:51        4      7            1            306.0   \n",
       "\n",
       "   request_length  \n",
       "0              67  \n",
       "1              16  \n",
       "2              59  \n",
       "3              30  \n",
       "4             103  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful downvotes average: 1775.81991952\n",
      "successful downvotes stddev: 18470.728594117507\n",
      "unsuccessful downvotes average: 1133.5554826\n",
      "unsuccessful downvotes stddev: 7490.887224296757\n"
     ]
    }
   ],
   "source": [
    "print('successful downvotes average:', np.average(b[b['requester_received_pizza'] == True]['total_downvotes']))\n",
    "print('successful downvotes stddev:', np.std(b[b['requester_received_pizza'] == True]['total_downvotes']))\n",
    "print('unsuccessful downvotes average:', np.average(b[b['requester_received_pizza'] == False]['total_downvotes']))\n",
    "print('unsuccessful downvotes stddev:', np.std(b[b['requester_received_pizza'] == False]['total_downvotes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133.5554826001314"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
