{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Project Final - Spring 2017\n",
    "\n",
    "### **Group:**\n",
    "\n",
    "Nichole Haas\n",
    "\n",
    "Jayashree Raman\n",
    "\n",
    "David Skarbrevik\n",
    "\n",
    "### **Dataset:**\n",
    "\n",
    "\"Random Acts of Pizza\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "print(sys.version)\n",
    "\n",
    "#make the depreciation warnings go away\n",
    "import warnings\n",
    "#I'm tired of the warnings on functions the professor asks us to use :) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get the training data\n",
    "train_json_array = np.array(pd.read_json('train.json', orient='columns'))\n",
    "#make it sklearn-able\n",
    "data_set = train_json_array[:,6]\n",
    "data_set_title = train_json_array[:,8]\n",
    "data_labels = train_json_array[:,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 giver_username_if_known\n",
      "1 number_of_downvotes_of_request_at_retrieval\n",
      "2 number_of_upvotes_of_request_at_retrieval\n",
      "3 post_was_edited\n",
      "4 request_id\n",
      "5 request_number_of_comments_at_retrieval\n",
      "6 request_text\n",
      "7 request_text_edit_aware\n",
      "8 request_title\n",
      "9 requester_account_age_in_days_at_request\n",
      "10 requester_account_age_in_days_at_retrieval\n",
      "11 requester_days_since_first_post_on_raop_at_request\n",
      "12 requester_days_since_first_post_on_raop_at_retrieval\n",
      "13 requester_number_of_comments_at_request\n",
      "14 requester_number_of_comments_at_retrieval\n",
      "15 requester_number_of_comments_in_raop_at_request\n",
      "16 requester_number_of_comments_in_raop_at_retrieval\n",
      "17 requester_number_of_posts_at_request\n",
      "18 requester_number_of_posts_at_retrieval\n",
      "19 requester_number_of_posts_on_raop_at_request\n",
      "20 requester_number_of_posts_on_raop_at_retrieval\n",
      "21 requester_number_of_subreddits_at_request\n",
      "22 requester_received_pizza\n",
      "23 requester_subreddits_at_request\n",
      "24 requester_upvotes_minus_downvotes_at_request\n",
      "25 requester_upvotes_minus_downvotes_at_retrieval\n",
      "26 requester_upvotes_plus_downvotes_at_request\n",
      "27 requester_upvotes_plus_downvotes_at_retrieval\n",
      "28 requester_user_flair\n",
      "29 requester_username\n",
      "30 unix_timestamp_of_request\n",
      "31 unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_json('train.json')\n",
    "for x, y in enumerate(a.columns):\n",
    "    print(x,y)\n",
    "#data_set = a['request_text']\n",
    "#data_set_title = a['request_title']\n",
    "#data_labels = a['requester_received_pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set = a[['request_text_edit_aware','request_title']]\n",
    "#data_set_title = a['request_title']\n",
    "data_labels = a['requester_received_pizza']\n",
    "clf = Pipeline([\n",
    "    ('counter', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())])\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3030, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    762\n",
       "True     248\n",
       "Name: requester_received_pizza, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2284\n",
       "True      746\n",
       "Name: requester_received_pizza, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "#try the Pipeline implementation from sk learn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "                ('cv', CountVectorizer(analyzer='char_wb', \n",
    "                                       #vocabulary=vocabulary,\n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('get-request', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "                ('cv', CountVectorizer(analyzer='char_wb', \n",
    "                                       #vocabulary=vocabulary,\n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "            \n",
    "           #('get-number', Pipeline([\n",
    "           #     ('selector', ItemSelector(key='number'))\n",
    "           # ]))\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('nb',LogisticRegression(penalty='l2',C=100)),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Got pizza       0.77      0.79      0.78       762\n",
      "   No pizza       0.31      0.29      0.30       248\n",
      "\n",
      "avg / total       0.66      0.67      0.66      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "target_names = ['Got pizza', 'No pizza']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4040, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = re.sub(r'[^a-zA-Z_0-9_\\s]',\" \", x).lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "                ('cv', CountVectorizer(preprocessor = preprocess,\n",
    "                                       analyzer='char_wb', \n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('get-request', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "                ('cv', CountVectorizer(preprocessor = preprocess, \n",
    "                                       analyzer='char_wb', \n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "            \n",
    "           #('get-number', Pipeline([\n",
    "           #     ('selector', ItemSelector(key='number'))\n",
    "           # ]))\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('nb',LogisticRegression(penalty='l2',C=100)),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.76      0.76       762\n",
      "       True       0.27      0.27      0.27       248\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I had hypothesized earlier that emojis would matter, so we ought not to \n",
    "# strip them.  the preprocessor used above strips emojis, and you can see\n",
    "# the predictor didn't perfom as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = x.lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "                ('cv', CountVectorizer(preprocessor = preprocess,\n",
    "                                       analyzer='char_wb', \n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('get-request', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "                ('cv', CountVectorizer(preprocessor = preprocess, \n",
    "                                       analyzer='char_wb', \n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "            \n",
    "           #('get-number', Pipeline([\n",
    "           #     ('selector', ItemSelector(key='number'))\n",
    "           # ]))\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('nb',LogisticRegression(penalty='l2',C=100)),\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.78      0.77       762\n",
      "       True       0.29      0.28      0.29       248\n",
      "\n",
      "avg / total       0.65      0.66      0.65      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'candy'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nope, the first one with no preprocessing works best\n",
    "#the f1 score is good for not getting pizza but bad for getting pizza\n",
    "#this tells us that more than the text is what matters when a pizza\n",
    "#wish is granted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 giver_username_if_known\n",
      "1 request_id\n",
      "2 request_text_edit_aware\n",
      "3 request_title\n",
      "4 requester_account_age_in_days_at_request\n",
      "5 requester_days_since_first_post_on_raop_at_request\n",
      "6 requester_number_of_comments_at_request\n",
      "7 requester_number_of_comments_in_raop_at_request\n",
      "8 requester_number_of_posts_at_request\n",
      "9 requester_number_of_posts_on_raop_at_request\n",
      "10 requester_number_of_subreddits_at_request\n",
      "11 requester_subreddits_at_request\n",
      "12 requester_upvotes_minus_downvotes_at_request\n",
      "13 requester_upvotes_plus_downvotes_at_request\n",
      "14 requester_username\n",
      "15 unix_timestamp_of_request\n",
      "16 unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_json('test.json')\n",
    "for x,y in enumerate(test.columns):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_fin_test = test[['request_text_edit_aware', 'request_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.79      0.78       762\n",
      "       True       0.31      0.29      0.30       248\n",
      "\n",
      "avg / total       0.66      0.67      0.66      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred1 = pipeline.predict(x_fin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>Just finished up the last of my classes and ru...</td>\n",
       "      <td>[REQUEST] Toronto Ontario. Broke student could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>This is my first time requesting for a pizza. ...</td>\n",
       "      <td>[REQUEST] GF of 5 years just broke up with me....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Hi guys! \\n\\nI'm a student,  typically strappe...</td>\n",
       "      <td>[REQUEST] Bit tight on cash, but more for a la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>It's been a very tight two weeks, and the wife...</td>\n",
       "      <td>[Request]Columbia, SC. Two chicken breasts and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>I just got home from work, and im broke, not e...</td>\n",
       "      <td>[Request] Home from work and tired in Henderso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                request_text_edit_aware  \\\n",
       "1177  Just finished up the last of my classes and ru...   \n",
       "2637  This is my first time requesting for a pizza. ...   \n",
       "1915  Hi guys! \\n\\nI'm a student,  typically strappe...   \n",
       "1091  It's been a very tight two weeks, and the wife...   \n",
       "1926  I just got home from work, and im broke, not e...   \n",
       "\n",
       "                                          request_title  \n",
       "1177  [REQUEST] Toronto Ontario. Broke student could...  \n",
       "2637  [REQUEST] GF of 5 years just broke up with me....  \n",
       "1915  [REQUEST] Bit tight on cash, but more for a la...  \n",
       "1091  [Request]Columbia, SC. Two chicken breasts and...  \n",
       "1926  [Request] Home from work and tired in Henderso...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>request_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             request_text_edit_aware  \\\n",
       "0  Hey all! It's about 95 degrees here and our ki...   \n",
       "1  I didn't know a place like this exists! \\n\\nI ...   \n",
       "2  Hi Reddit. Im a single dad having a really rou...   \n",
       "3  Hi I just moved to Waltham MA from my home sta...   \n",
       "4  We're just sitting here near indianapolis on o...   \n",
       "\n",
       "                                       request_title  \n",
       "0  [request] pregger gf 95 degree house and no fo...  \n",
       "1  [Request] Lost my job day after labour day, st...  \n",
       "2                (Request) pizza for my kids please?  \n",
       "3  [Request] Just moved to a new state(Waltham MA...  \n",
       "4  [Request] Two girls in between paychecks, we'v...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fin_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False], dtype=bool)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inter(x):\n",
    "    return int(x)\n",
    "vint = np.vectorize(inter)\n",
    "pred2 = vint(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin_df = pd.concat([test['request_id'],pd.DataFrame(pred2, columns=['requester_received_pizza'])],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1631, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_df.to_csv('pizza_submission.csv', sep = ',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
