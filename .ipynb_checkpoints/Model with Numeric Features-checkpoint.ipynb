{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "#make the depreciation warnings go away\n",
    "import warnings\n",
    "#I'm tired of the warnings on functions the professor asks us to use :) \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "print(sys.version)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 giver_username_if_known\n",
      "1 number_of_downvotes_of_request_at_retrieval\n",
      "2 number_of_upvotes_of_request_at_retrieval\n",
      "3 post_was_edited\n",
      "4 request_id\n",
      "5 request_number_of_comments_at_retrieval\n",
      "6 request_text\n",
      "7 request_text_edit_aware\n",
      "8 request_title\n",
      "9 requester_account_age_in_days_at_request\n",
      "10 requester_account_age_in_days_at_retrieval\n",
      "11 requester_days_since_first_post_on_raop_at_request\n",
      "12 requester_days_since_first_post_on_raop_at_retrieval\n",
      "13 requester_number_of_comments_at_request\n",
      "14 requester_number_of_comments_at_retrieval\n",
      "15 requester_number_of_comments_in_raop_at_request\n",
      "16 requester_number_of_comments_in_raop_at_retrieval\n",
      "17 requester_number_of_posts_at_request\n",
      "18 requester_number_of_posts_at_retrieval\n",
      "19 requester_number_of_posts_on_raop_at_request\n",
      "20 requester_number_of_posts_on_raop_at_retrieval\n",
      "21 requester_number_of_subreddits_at_request\n",
      "22 requester_received_pizza\n",
      "23 requester_subreddits_at_request\n",
      "24 requester_upvotes_minus_downvotes_at_request\n",
      "25 requester_upvotes_minus_downvotes_at_retrieval\n",
      "26 requester_upvotes_plus_downvotes_at_request\n",
      "27 requester_upvotes_plus_downvotes_at_retrieval\n",
      "28 requester_user_flair\n",
      "29 requester_username\n",
      "30 unix_timestamp_of_request\n",
      "31 unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_json('train.json')\n",
    "for x, y in enumerate(a.columns):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nikki's first three hypotheses: date/time will matter\n",
    "\n",
    "import datetime\n",
    "def day_time(x):\n",
    "    y = ''\n",
    "    if datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 10:\n",
    "        y = 0\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 10 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 16:\n",
    "        y = 1\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 16 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 21:\n",
    "        y = 2\n",
    "    else: \n",
    "        y = 3\n",
    "    return y\n",
    "\n",
    "day_values = ['morning', 'midday', 'evening', 'latenight']\n",
    "    \n",
    "def human_time(a):\n",
    "    import datetime\n",
    "    from datetime import date\n",
    "    import calendar\n",
    "    ### for the data in raop, return human time.  maybe the time of day matters\n",
    "    a['human_readable_local_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['human_readable_UTC_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['weekday'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).weekday()\n",
    "    a['month'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).month\n",
    "    a['time_of_day'] = day_time(a)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.apply(human_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### add in number of downvotes.  The test data doesn't have this, it only has these aggregate values.  Derive number\n",
    "### of downvotes\n",
    "### we're given \n",
    "###up + down = col_26 \n",
    "###and \n",
    "### up-down = col_24\n",
    "### solve algebraically,\n",
    "# 2up = col_26+col_24,\n",
    "# up = (col_26+col_24)/2\n",
    "# (col_26+col_24)/2 + down = col_26\n",
    "#down = col_26 - (col_26+col_24)/2\n",
    "#down = (col_26 - col_24)/2\n",
    "\n",
    "b['total_downvotes'] = (b['requester_upvotes_plus_downvotes_at_request'] - b['requester_upvotes_minus_downvotes_at_request'])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = re.sub(r'[^a-zA-Z_0-9_\\s]',\" \", x).lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "#try the Pipeline implementation from sk learn\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "#from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class WeekdayExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        shape = data_dict.shape\n",
    "        return data_dict[self.key].reshape(shape[0],1)\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "class NBWrapper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "    def __init__(self, NB):\n",
    "        self.NB = NB\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self.NB.fit(x, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return self.NB.predict(posts)\n",
    "\n",
    "pipeline4= Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        ### add in text variables\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "           #     ('cv', CountVectorizer(analyzer='char_wb', \n",
    "           #                           #vocabulary=vocabulary,\n",
    "           #                            #preprocessor=preprocess,\n",
    "           #                          max_df=0.5, ngram_range=(1,2))),\n",
    "                ('tfidf', TfidfVectorizer(min_df=65, \n",
    "                                          preprocessor=preprocess\n",
    "                                         ))\n",
    "            ])),\n",
    "\n",
    "            \n",
    "            #('get-text', Pipeline([\n",
    "            #    ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "            #    ('tfidf', TfidfVectorizer(min_df=50))\n",
    "                #('cv', CountVectorizer(analyzer='char_wb', \n",
    "                #                       #vocabulary=vocabulary,\n",
    "                #                       max_df=0.5, ngram_range=(1,3)))\n",
    "            #])),\n",
    "         ### adding in categorical variables\n",
    "           # research different encoders\n",
    "           ('get-time_of_day', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='time_of_day')),\n",
    "                ('one_hot', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "        \n",
    "           ('get-weekday', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='weekday')),\n",
    "                ('lh', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "          ('get-month', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='month')),\n",
    "                ('2h', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "                    \n",
    "                    \n",
    "        ### add numeric data\n",
    "        #research better numeric extractors and coders\n",
    "         ('get-requester_number_of_subreddits_at_request', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='requester_number_of_subreddits_at_request')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))             \n",
    "           ])),\n",
    "            \n",
    "         ('get-downvotes', Pipeline([\n",
    "                ('selector', WeekdayExtractor(key='total_downvotes')),\n",
    "                #('lb', OneHotEncoder())\n",
    "                #('NB', NBWrapper(MultinomialNB()))   \n",
    "                \n",
    "           ]))\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    #('estimator',LogisticRegression(penalty='l2',C=25))\n",
    "    #('estimator', SVC(kernel='linear')) \n",
    "    \n",
    "    ('estimator', DecisionTreeClassifier())\n",
    "    #('estimator', RandomForestClassifier())\n",
    "    #('estimator', MultinomialNB())\n",
    "    #('estmator', KNeighborsClassifier())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.77      0.80      0.78       762\n",
      "  Got Pizza       0.32      0.29      0.30       248\n",
      "\n",
      "avg / total       0.66      0.67      0.67      1010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = b[['request_title','requester_number_of_subreddits_at_request', \n",
    "              'time_of_day','weekday', 'month', 'total_downvotes']]\n",
    "#data_set_title = a['request_title']\n",
    "data_labels = b['requester_received_pizza']\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]\n",
    "\n",
    "pipeline4.fit(X_train, y_train)\n",
    "y_pred = pipeline4.predict(X_test)\n",
    "target_names = ['No pizza','Got Pizza']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[606, 176],\n",
       "       [156,  72]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
