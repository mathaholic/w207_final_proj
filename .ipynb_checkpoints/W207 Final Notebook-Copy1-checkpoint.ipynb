{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project: Random Acts of Pizza\n",
    "\n",
    "#### By:  David Skarbrevik, Jayashree Ramen, and Nikki Haas\n",
    "#### MIDS W207:  Intro to Machine Learning\n",
    "#### University of California, Berkeley\n",
    "#### Spring, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.3.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#some of the SKLearn features we learnt in class are being depreciated.  They still exist for now, so let's ignore \n",
    "#the watnings\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "#General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from subprocess import check_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "#SkLearn construction items\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning / Manipulating the Data / EDA / Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features in Training Data\n",
      "0 giver_username_if_known\n",
      "1 number_of_downvotes_of_request_at_retrieval\n",
      "2 number_of_upvotes_of_request_at_retrieval\n",
      "3 post_was_edited\n",
      "4 request_id\n",
      "5 request_number_of_comments_at_retrieval\n",
      "6 request_text\n",
      "7 request_text_edit_aware\n",
      "8 request_title\n",
      "9 requester_account_age_in_days_at_request\n",
      "10 requester_account_age_in_days_at_retrieval\n",
      "11 requester_days_since_first_post_on_raop_at_request\n",
      "12 requester_days_since_first_post_on_raop_at_retrieval\n",
      "13 requester_number_of_comments_at_request\n",
      "14 requester_number_of_comments_at_retrieval\n",
      "15 requester_number_of_comments_in_raop_at_request\n",
      "16 requester_number_of_comments_in_raop_at_retrieval\n",
      "17 requester_number_of_posts_at_request\n",
      "18 requester_number_of_posts_at_retrieval\n",
      "19 requester_number_of_posts_on_raop_at_request\n",
      "20 requester_number_of_posts_on_raop_at_retrieval\n",
      "21 requester_number_of_subreddits_at_request\n",
      "22 requester_received_pizza\n",
      "23 requester_subreddits_at_request\n",
      "24 requester_upvotes_minus_downvotes_at_request\n",
      "25 requester_upvotes_minus_downvotes_at_retrieval\n",
      "26 requester_upvotes_plus_downvotes_at_request\n",
      "27 requester_upvotes_plus_downvotes_at_retrieval\n",
      "28 requester_user_flair\n",
      "29 requester_username\n",
      "30 unix_timestamp_of_request\n",
      "31 unix_timestamp_of_request_utc\n",
      "\n",
      "\n",
      "All Features in Test Data (fewer than Training Data)\n",
      "0 giver_username_if_known\n",
      "1 request_id\n",
      "2 request_text_edit_aware\n",
      "3 request_title\n",
      "4 requester_account_age_in_days_at_request\n",
      "5 requester_days_since_first_post_on_raop_at_request\n",
      "6 requester_number_of_comments_at_request\n",
      "7 requester_number_of_comments_in_raop_at_request\n",
      "8 requester_number_of_posts_at_request\n",
      "9 requester_number_of_posts_on_raop_at_request\n",
      "10 requester_number_of_subreddits_at_request\n",
      "11 requester_subreddits_at_request\n",
      "12 requester_upvotes_minus_downvotes_at_request\n",
      "13 requester_upvotes_plus_downvotes_at_request\n",
      "14 requester_username\n",
      "15 unix_timestamp_of_request\n",
      "16 unix_timestamp_of_request_utc\n"
     ]
    }
   ],
   "source": [
    "### data needed:\n",
    "#binarized categorical data: df6_data \n",
    "#numeric data: num_data\n",
    "#numeric data cannot be used as is.  \n",
    "#Take num_data and normalize all fields\n",
    "#text data:  data_set\n",
    "#text data's pipeline masks some of the data, let's redo the Tf-Idf vectorization\n",
    "\n",
    "train_data = pd.read_json('train.json', orient='columns')\n",
    "train_labels = train_data['requester_received_pizza']\n",
    "test_data = pd.read_json('test.json', orient='columns')\n",
    "\n",
    "print(\"All Features in Training Data\")\n",
    "for x,y in enumerate(train_data.columns):\n",
    "    print(x,y)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"All Features in Test Data (fewer than Training Data)\")\n",
    "for x,y in enumerate(test_data.columns):\n",
    "    print(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Binarized Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def day_time(x):\n",
    "    y = ''\n",
    "    if datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 10:\n",
    "        y = 'morning'\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 10 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 16:\n",
    "        y = 'midday'\n",
    "    elif datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour >= 16 and datetime.datetime.fromtimestamp(x['unix_timestamp_of_request_utc']).hour < 21:\n",
    "        y = 'evening'\n",
    "    else: \n",
    "        y = 'late_night'\n",
    "    return y\n",
    "\n",
    "day_values = ['morning', 'midday', 'evening', 'late_night']\n",
    "    \n",
    "def human_time(a):\n",
    "    import datetime\n",
    "    from datetime import date\n",
    "    import calendar\n",
    "    ### for the data in raop, return human time.  maybe the time of day matters\n",
    "    a['human_readable_local_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['human_readable_UTC_time'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    a['weekday'] = calendar.day_name[datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).weekday()].lower()\n",
    "    a['month'] = datetime.datetime.fromtimestamp(a['unix_timestamp_of_request']).strftime(\"%B\").lower()\n",
    "    a['time_of_day'] = day_time(a)\n",
    "    return a\n",
    "\n",
    "a = pd.read_json('train.json')\n",
    "b = a.apply(human_time, axis=1)\n",
    "c = pd.read_json('test.json')\n",
    "d = c.apply(human_time, axis =1) \n",
    "\n",
    "#parse the training set to match the testing set's columns\n",
    "df = b[list(d.columns)]\n",
    "tstdf = d[list(d.columns)]\n",
    "\n",
    "b = pd.get_dummies(df['weekday'])\n",
    "tstb = pd.get_dummies(tstdf['weekday'])\n",
    "c = pd.get_dummies(df['time_of_day'])\n",
    "tstc = pd.get_dummies(tstdf['time_of_day'])\n",
    "d= pd.get_dummies(df['month'])\n",
    "tstd= pd.get_dummies(tstdf['month'])\n",
    "df2 = pd.concat([df,b,c,d], axis=1)\n",
    "tstdf2 = pd.concat([tstdf,tstb,tstc,tstd], axis=1)\n",
    "\n",
    "del df2['month']\n",
    "del tstdf2['month']\n",
    "del df2['weekday']\n",
    "del tstdf2['weekday']\n",
    "del df2['time_of_day']\n",
    "del tstdf2['time_of_day']\n",
    "\n",
    "df4 = pd.get_dummies(df2['requester_subreddits_at_request'].apply(pd.Series), prefix='', prefix_sep='').sum(level=0, axis=1)\n",
    "tstdf4 = pd.get_dummies(tstdf2['requester_subreddits_at_request'].apply(pd.Series), prefix='', prefix_sep='').sum(level=0, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df2 = df2[[ 'request_id','friday',\n",
    "       'monday', 'saturday', 'sunday', 'thursday', 'tuesday', 'wednesday',\n",
    "       'evening', 'late_night', 'midday', 'morning', 'april', 'august',\n",
    "       'december', 'february', 'january', 'july', 'june', 'march', 'may',\n",
    "       'november', 'october', 'september']]\n",
    "tstdf2 = tstdf2[[ 'request_id','friday',\n",
    "       'monday', 'saturday', 'sunday', 'thursday', 'tuesday', 'wednesday',\n",
    "       'evening', 'late_night', 'midday', 'morning', 'april', 'august',\n",
    "       'december', 'february', 'january', 'july', 'june', 'march', 'may',\n",
    "       'november', 'october', 'september']]\n",
    "\n",
    "df5 = pd.concat([a['requester_received_pizza'],df2,df4], axis = 1)\n",
    "tstdf5 = pd.concat([tstdf2,tstdf4], axis = 1)\n",
    "\n",
    "del df5['request_id']\n",
    "del tstdf5['request_id']\n",
    "\n",
    "df6_labels = df5['requester_received_pizza']\n",
    "del df5['requester_received_pizza']\n",
    "df6_data = df5\n",
    "\n",
    "bincat_train_data = df6_data[0:2019]\n",
    "bincat_train_labels = df6_labels[0:2019]\n",
    "bincat_test_data = df6_data[2020:]\n",
    "bincat_test_labels = df6_labels[2020:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten vote counts for posts: \n",
      " [1286864, 789287, 237572, 194891, 184831, 149645, 139199, 110450, 107283, 106518]\n",
      "\n",
      " Are there any missing values?: False\n"
     ]
    }
   ],
   "source": [
    "# some posts have crazy high vote counts, should consider normalization\n",
    "print(\"Top ten vote counts for posts: \\n\", list(np.sort(train_data.iloc[:,26])[:-11:-1]))\n",
    "\n",
    "# indices of all numeric variables (can't use this in models, b/c not all features will be in final test data)\n",
    "all_count_num_vars = [1,2,5,9,10,11,12,13,14,15,16,17,18,19,20,21,24,25,26,27]\n",
    "\n",
    "#numeric data that is only is in test data (excludes numerics that are only in train data)\n",
    "cols = ['requester_account_age_in_days_at_request',\n",
    "       'requester_days_since_first_post_on_raop_at_request',\n",
    "       'requester_number_of_comments_at_request',\n",
    "       'requester_number_of_comments_in_raop_at_request',\n",
    "       'requester_number_of_posts_at_request',\n",
    "       'requester_number_of_posts_on_raop_at_request',\n",
    "       'requester_number_of_subreddits_at_request','requester_upvotes_minus_downvotes_at_request',\n",
    "       'requester_upvotes_plus_downvotes_at_request']\n",
    "\n",
    "# subsetting all numeric variables in test data\n",
    "#num_data = train_data.iloc[:,all_count_num_vars]\n",
    "num_data = train_data[cols]\n",
    "\n",
    "\n",
    "# are there any NaNs?\n",
    "print(\"\\n Are there any missing values?:\", num_data.isnull().values.any())\n",
    "\n",
    "\n",
    "# normalization of numeric data for later models\n",
    "scl = MaxAbsScaler()\n",
    "num_norm = pd.DataFrame(index=num_data.index)\n",
    "\n",
    "for col in cols:\n",
    "    scl.fit(num_data[col].values.reshape(-1,1))\n",
    "    c = col+'_scaled'\n",
    "    num_norm[c] = scl.transform(num_data[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:25: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "C:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:26: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Parameters 4 PCA Comp 1 GMM Comp 1 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "2 Parameters 4 PCA Comp 1 GMM Comp 1 Cov Matrix diag Accuracy is\t 52.48%\n",
      "3 Parameters 4 PCA Comp 1 GMM Comp 1 Cov Matrix tied Accuracy is\t 52.48%\n",
      "4 Parameters 4 PCA Comp 1 GMM Comp 1 Cov Matrix full Accuracy is\t 52.48%\n",
      "5 Parameters 6 PCA Comp 1 GMM Comp 2 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "6 Parameters 8 PCA Comp 1 GMM Comp 2 Cov Matrix diag Accuracy is\t 52.48%\n",
      "7 Parameters 6 PCA Comp 1 GMM Comp 2 Cov Matrix tied Accuracy is\t 52.48%\n",
      "8 Parameters 8 PCA Comp 1 GMM Comp 2 Cov Matrix full Accuracy is\t 52.48%\n",
      "9 Parameters 8 PCA Comp 1 GMM Comp 3 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "10 Parameters 12 PCA Comp 1 GMM Comp 3 Cov Matrix diag Accuracy is\t 52.48%\n",
      "11 Parameters 8 PCA Comp 1 GMM Comp 3 Cov Matrix tied Accuracy is\t 52.48%\n",
      "12 Parameters 12 PCA Comp 1 GMM Comp 3 Cov Matrix full Accuracy is\t 52.48%\n",
      "13 Parameters 10 PCA Comp 1 GMM Comp 4 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "14 Parameters 16 PCA Comp 1 GMM Comp 4 Cov Matrix diag Accuracy is\t 52.48%\n",
      "15 Parameters 10 PCA Comp 1 GMM Comp 4 Cov Matrix tied Accuracy is\t 52.48%\n",
      "16 Parameters 16 PCA Comp 1 GMM Comp 4 Cov Matrix full Accuracy is\t 52.48%\n",
      "17 Parameters 12 PCA Comp 1 GMM Comp 5 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "18 Parameters 20 PCA Comp 1 GMM Comp 5 Cov Matrix diag Accuracy is\t 52.48%\n",
      "19 Parameters 12 PCA Comp 1 GMM Comp 5 Cov Matrix tied Accuracy is\t 52.48%\n",
      "20 Parameters 20 PCA Comp 1 GMM Comp 5 Cov Matrix full Accuracy is\t 52.48%\n",
      "21 Parameters 14 PCA Comp 1 GMM Comp 6 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "22 Parameters 24 PCA Comp 1 GMM Comp 6 Cov Matrix diag Accuracy is\t 52.48%\n",
      "23 Parameters 14 PCA Comp 1 GMM Comp 6 Cov Matrix tied Accuracy is\t 52.48%\n",
      "24 Parameters 24 PCA Comp 1 GMM Comp 6 Cov Matrix full Accuracy is\t 52.48%\n",
      "25 Parameters 16 PCA Comp 1 GMM Comp 7 Cov Matrix spherical Accuracy is\t 52.48%\n",
      "26 Parameters 28 PCA Comp 1 GMM Comp 7 Cov Matrix diag Accuracy is\t 52.48%\n",
      "27 Parameters 16 PCA Comp 1 GMM Comp 7 Cov Matrix tied Accuracy is\t 52.48%\n",
      "28 Parameters 28 PCA Comp 1 GMM Comp 7 Cov Matrix full Accuracy is\t 52.48%\n",
      "29 Parameters 6 PCA Comp 2 GMM Comp 1 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "30 Parameters 8 PCA Comp 2 GMM Comp 1 Cov Matrix diag Accuracy is\t 62.62%\n",
      "31 Parameters 10 PCA Comp 2 GMM Comp 1 Cov Matrix tied Accuracy is\t 60.40%\n",
      "32 Parameters 10 PCA Comp 2 GMM Comp 1 Cov Matrix full Accuracy is\t 60.40%\n",
      "33 Parameters 10 PCA Comp 2 GMM Comp 2 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "34 Parameters 16 PCA Comp 2 GMM Comp 2 Cov Matrix diag Accuracy is\t 60.40%\n",
      "35 Parameters 14 PCA Comp 2 GMM Comp 2 Cov Matrix tied Accuracy is\t 60.40%\n",
      "36 Parameters 20 PCA Comp 2 GMM Comp 2 Cov Matrix full Accuracy is\t 60.40%\n",
      "37 Parameters 14 PCA Comp 2 GMM Comp 3 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "38 Parameters 24 PCA Comp 2 GMM Comp 3 Cov Matrix diag Accuracy is\t 60.40%\n",
      "39 Parameters 18 PCA Comp 2 GMM Comp 3 Cov Matrix tied Accuracy is\t 60.40%\n",
      "40 Parameters 30 PCA Comp 2 GMM Comp 3 Cov Matrix full Accuracy is\t 60.40%\n",
      "41 Parameters 18 PCA Comp 2 GMM Comp 4 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "42 Parameters 32 PCA Comp 2 GMM Comp 4 Cov Matrix diag Accuracy is\t 60.40%\n",
      "43 Parameters 22 PCA Comp 2 GMM Comp 4 Cov Matrix tied Accuracy is\t 60.40%\n",
      "44 Parameters 40 PCA Comp 2 GMM Comp 4 Cov Matrix full Accuracy is\t 60.40%\n",
      "45 Parameters 22 PCA Comp 2 GMM Comp 5 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "46 Parameters 40 PCA Comp 2 GMM Comp 5 Cov Matrix diag Accuracy is\t 60.40%\n",
      "47 Parameters 26 PCA Comp 2 GMM Comp 5 Cov Matrix tied Accuracy is\t 60.40%\n",
      "48 Parameters 50 PCA Comp 2 GMM Comp 5 Cov Matrix full Accuracy is\t 60.40%\n",
      "49 Parameters 26 PCA Comp 2 GMM Comp 6 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "50 Parameters 48 PCA Comp 2 GMM Comp 6 Cov Matrix diag Accuracy is\t 60.40%\n",
      "51 Parameters 30 PCA Comp 2 GMM Comp 6 Cov Matrix tied Accuracy is\t 60.40%\n",
      "52 Parameters 60 PCA Comp 2 GMM Comp 6 Cov Matrix full exceeds 50 params\n",
      "53 Parameters 30 PCA Comp 2 GMM Comp 7 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "54 Parameters 56 PCA Comp 2 GMM Comp 7 Cov Matrix diag exceeds 50 params\n",
      "55 Parameters 34 PCA Comp 2 GMM Comp 7 Cov Matrix tied Accuracy is\t 60.40%\n",
      "56 Parameters 70 PCA Comp 2 GMM Comp 7 Cov Matrix full exceeds 50 params\n",
      "57 Parameters 8 PCA Comp 3 GMM Comp 1 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "58 Parameters 12 PCA Comp 3 GMM Comp 1 Cov Matrix diag Accuracy is\t 57.62%\n",
      "59 Parameters 18 PCA Comp 3 GMM Comp 1 Cov Matrix tied Accuracy is\t 60.59%\n",
      "60 Parameters 18 PCA Comp 3 GMM Comp 1 Cov Matrix full Accuracy is\t 60.59%\n",
      "61 Parameters 14 PCA Comp 3 GMM Comp 2 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "62 Parameters 24 PCA Comp 3 GMM Comp 2 Cov Matrix diag Accuracy is\t 60.59%\n",
      "63 Parameters 24 PCA Comp 3 GMM Comp 2 Cov Matrix tied Accuracy is\t 60.59%\n",
      "64 Parameters 36 PCA Comp 3 GMM Comp 2 Cov Matrix full Accuracy is\t 60.59%\n",
      "65 Parameters 20 PCA Comp 3 GMM Comp 3 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "66 Parameters 36 PCA Comp 3 GMM Comp 3 Cov Matrix diag Accuracy is\t 60.59%\n",
      "67 Parameters 30 PCA Comp 3 GMM Comp 3 Cov Matrix tied Accuracy is\t 60.59%\n",
      "68 Parameters 54 PCA Comp 3 GMM Comp 3 Cov Matrix full exceeds 50 params\n",
      "69 Parameters 26 PCA Comp 3 GMM Comp 4 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "70 Parameters 48 PCA Comp 3 GMM Comp 4 Cov Matrix diag Accuracy is\t 60.59%\n",
      "71 Parameters 36 PCA Comp 3 GMM Comp 4 Cov Matrix tied Accuracy is\t 60.59%\n",
      "72 Parameters 72 PCA Comp 3 GMM Comp 4 Cov Matrix full exceeds 50 params\n",
      "73 Parameters 32 PCA Comp 3 GMM Comp 5 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "74 Parameters 60 PCA Comp 3 GMM Comp 5 Cov Matrix diag exceeds 50 params\n",
      "75 Parameters 42 PCA Comp 3 GMM Comp 5 Cov Matrix tied Accuracy is\t 60.59%\n",
      "76 Parameters 90 PCA Comp 3 GMM Comp 5 Cov Matrix full exceeds 50 params\n",
      "77 Parameters 38 PCA Comp 3 GMM Comp 6 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "78 Parameters 72 PCA Comp 3 GMM Comp 6 Cov Matrix diag exceeds 50 params\n",
      "79 Parameters 48 PCA Comp 3 GMM Comp 6 Cov Matrix tied Accuracy is\t 60.59%\n",
      "80 Parameters 108 PCA Comp 3 GMM Comp 6 Cov Matrix full exceeds 50 params\n",
      "81 Parameters 44 PCA Comp 3 GMM Comp 7 Cov Matrix spherical Accuracy is\t 60.59%\n",
      "82 Parameters 84 PCA Comp 3 GMM Comp 7 Cov Matrix diag exceeds 50 params\n",
      "83 Parameters 54 PCA Comp 3 GMM Comp 7 Cov Matrix tied exceeds 50 params\n",
      "84 Parameters 126 PCA Comp 3 GMM Comp 7 Cov Matrix full exceeds 50 params\n",
      "85 Parameters 10 PCA Comp 4 GMM Comp 1 Cov Matrix spherical Accuracy is\t 60.84%\n",
      "86 Parameters 16 PCA Comp 4 GMM Comp 1 Cov Matrix diag Accuracy is\t 59.26%\n",
      "87 Parameters 28 PCA Comp 4 GMM Comp 1 Cov Matrix tied Accuracy is\t 60.84%\n",
      "88 Parameters 28 PCA Comp 4 GMM Comp 1 Cov Matrix full Accuracy is\t 60.84%\n",
      "89 Parameters 18 PCA Comp 4 GMM Comp 2 Cov Matrix spherical Accuracy is\t 60.84%\n",
      "90 Parameters 32 PCA Comp 4 GMM Comp 2 Cov Matrix diag Accuracy is\t 60.84%\n",
      "91 Parameters 36 PCA Comp 4 GMM Comp 2 Cov Matrix tied Accuracy is\t 60.84%\n",
      "92 Parameters 56 PCA Comp 4 GMM Comp 2 Cov Matrix full exceeds 50 params\n",
      "93 Parameters 26 PCA Comp 4 GMM Comp 3 Cov Matrix spherical Accuracy is\t 60.84%\n",
      "94 Parameters 48 PCA Comp 4 GMM Comp 3 Cov Matrix diag Accuracy is\t 60.84%\n",
      "95 Parameters 44 PCA Comp 4 GMM Comp 3 Cov Matrix tied Accuracy is\t 60.84%\n",
      "96 Parameters 84 PCA Comp 4 GMM Comp 3 Cov Matrix full exceeds 50 params\n",
      "97 Parameters 34 PCA Comp 4 GMM Comp 4 Cov Matrix spherical Accuracy is\t 60.84%\n",
      "98 Parameters 64 PCA Comp 4 GMM Comp 4 Cov Matrix diag exceeds 50 params\n",
      "99 Parameters 52 PCA Comp 4 GMM Comp 4 Cov Matrix tied exceeds 50 params\n",
      "100 Parameters 112 PCA Comp 4 GMM Comp 4 Cov Matrix full exceeds 50 params\n",
      "101 Parameters 42 PCA Comp 4 GMM Comp 5 Cov Matrix spherical Accuracy is\t 60.84%\n",
      "102 Parameters 80 PCA Comp 4 GMM Comp 5 Cov Matrix diag exceeds 50 params\n",
      "103 Parameters 60 PCA Comp 4 GMM Comp 5 Cov Matrix tied exceeds 50 params\n",
      "104 Parameters 140 PCA Comp 4 GMM Comp 5 Cov Matrix full exceeds 50 params\n",
      "105 Parameters 50 PCA Comp 4 GMM Comp 6 Cov Matrix spherical Accuracy is\t 60.84%\n",
      "106 Parameters 96 PCA Comp 4 GMM Comp 6 Cov Matrix diag exceeds 50 params\n",
      "107 Parameters 68 PCA Comp 4 GMM Comp 6 Cov Matrix tied exceeds 50 params\n",
      "108 Parameters 168 PCA Comp 4 GMM Comp 6 Cov Matrix full exceeds 50 params\n",
      "109 Parameters 58 PCA Comp 4 GMM Comp 7 Cov Matrix spherical exceeds 50 params\n",
      "110 Parameters 112 PCA Comp 4 GMM Comp 7 Cov Matrix diag exceeds 50 params\n",
      "111 Parameters 76 PCA Comp 4 GMM Comp 7 Cov Matrix tied exceeds 50 params\n",
      "112 Parameters 196 PCA Comp 4 GMM Comp 7 Cov Matrix full exceeds 50 params\n",
      "113 Parameters 12 PCA Comp 5 GMM Comp 1 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "114 Parameters 20 PCA Comp 5 GMM Comp 1 Cov Matrix diag Accuracy is\t 56.88%\n",
      "115 Parameters 40 PCA Comp 5 GMM Comp 1 Cov Matrix tied Accuracy is\t 60.40%\n",
      "116 Parameters 40 PCA Comp 5 GMM Comp 1 Cov Matrix full Accuracy is\t 60.40%\n",
      "117 Parameters 22 PCA Comp 5 GMM Comp 2 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "118 Parameters 40 PCA Comp 5 GMM Comp 2 Cov Matrix diag Accuracy is\t 60.40%\n",
      "119 Parameters 50 PCA Comp 5 GMM Comp 2 Cov Matrix tied Accuracy is\t 60.40%\n",
      "120 Parameters 80 PCA Comp 5 GMM Comp 2 Cov Matrix full exceeds 50 params\n",
      "121 Parameters 32 PCA Comp 5 GMM Comp 3 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "122 Parameters 60 PCA Comp 5 GMM Comp 3 Cov Matrix diag exceeds 50 params\n",
      "123 Parameters 60 PCA Comp 5 GMM Comp 3 Cov Matrix tied exceeds 50 params\n",
      "124 Parameters 120 PCA Comp 5 GMM Comp 3 Cov Matrix full exceeds 50 params\n",
      "125 Parameters 42 PCA Comp 5 GMM Comp 4 Cov Matrix spherical Accuracy is\t 60.40%\n",
      "126 Parameters 80 PCA Comp 5 GMM Comp 4 Cov Matrix diag exceeds 50 params\n",
      "127 Parameters 70 PCA Comp 5 GMM Comp 4 Cov Matrix tied exceeds 50 params\n",
      "128 Parameters 160 PCA Comp 5 GMM Comp 4 Cov Matrix full exceeds 50 params\n",
      "129 Parameters 52 PCA Comp 5 GMM Comp 5 Cov Matrix spherical exceeds 50 params\n",
      "130 Parameters 100 PCA Comp 5 GMM Comp 5 Cov Matrix diag exceeds 50 params\n",
      "131 Parameters 80 PCA Comp 5 GMM Comp 5 Cov Matrix tied exceeds 50 params\n",
      "132 Parameters 200 PCA Comp 5 GMM Comp 5 Cov Matrix full exceeds 50 params\n",
      "133 Parameters 62 PCA Comp 5 GMM Comp 6 Cov Matrix spherical exceeds 50 params\n",
      "134 Parameters 120 PCA Comp 5 GMM Comp 6 Cov Matrix diag exceeds 50 params\n",
      "135 Parameters 90 PCA Comp 5 GMM Comp 6 Cov Matrix tied exceeds 50 params\n",
      "136 Parameters 240 PCA Comp 5 GMM Comp 6 Cov Matrix full exceeds 50 params\n",
      "137 Parameters 72 PCA Comp 5 GMM Comp 7 Cov Matrix spherical exceeds 50 params\n",
      "138 Parameters 140 PCA Comp 5 GMM Comp 7 Cov Matrix diag exceeds 50 params\n",
      "139 Parameters 100 PCA Comp 5 GMM Comp 7 Cov Matrix tied exceeds 50 params\n",
      "140 Parameters 280 PCA Comp 5 GMM Comp 7 Cov Matrix full exceeds 50 params\n",
      "141 Parameters 14 PCA Comp 6 GMM Comp 1 Cov Matrix spherical Accuracy is\t 61.78%\n",
      "142 Parameters 24 PCA Comp 6 GMM Comp 1 Cov Matrix diag Accuracy is\t 52.67%\n",
      "143 Parameters 54 PCA Comp 6 GMM Comp 1 Cov Matrix tied exceeds 50 params\n",
      "144 Parameters 54 PCA Comp 6 GMM Comp 1 Cov Matrix full exceeds 50 params\n",
      "145 Parameters 26 PCA Comp 6 GMM Comp 2 Cov Matrix spherical Accuracy is\t 61.78%\n",
      "146 Parameters 48 PCA Comp 6 GMM Comp 2 Cov Matrix diag Accuracy is\t 61.78%\n",
      "147 Parameters 66 PCA Comp 6 GMM Comp 2 Cov Matrix tied exceeds 50 params\n",
      "148 Parameters 108 PCA Comp 6 GMM Comp 2 Cov Matrix full exceeds 50 params\n",
      "149 Parameters 38 PCA Comp 6 GMM Comp 3 Cov Matrix spherical Accuracy is\t 61.78%\n",
      "150 Parameters 72 PCA Comp 6 GMM Comp 3 Cov Matrix diag exceeds 50 params\n",
      "151 Parameters 78 PCA Comp 6 GMM Comp 3 Cov Matrix tied exceeds 50 params\n",
      "152 Parameters 162 PCA Comp 6 GMM Comp 3 Cov Matrix full exceeds 50 params\n",
      "153 Parameters 50 PCA Comp 6 GMM Comp 4 Cov Matrix spherical Accuracy is\t 61.78%\n",
      "154 Parameters 96 PCA Comp 6 GMM Comp 4 Cov Matrix diag exceeds 50 params\n",
      "155 Parameters 90 PCA Comp 6 GMM Comp 4 Cov Matrix tied exceeds 50 params\n",
      "156 Parameters 216 PCA Comp 6 GMM Comp 4 Cov Matrix full exceeds 50 params\n",
      "157 Parameters 62 PCA Comp 6 GMM Comp 5 Cov Matrix spherical exceeds 50 params\n",
      "158 Parameters 120 PCA Comp 6 GMM Comp 5 Cov Matrix diag exceeds 50 params\n",
      "159 Parameters 102 PCA Comp 6 GMM Comp 5 Cov Matrix tied exceeds 50 params\n",
      "160 Parameters 270 PCA Comp 6 GMM Comp 5 Cov Matrix full exceeds 50 params\n",
      "161 Parameters 74 PCA Comp 6 GMM Comp 6 Cov Matrix spherical exceeds 50 params\n",
      "162 Parameters 144 PCA Comp 6 GMM Comp 6 Cov Matrix diag exceeds 50 params\n",
      "163 Parameters 114 PCA Comp 6 GMM Comp 6 Cov Matrix tied exceeds 50 params\n",
      "164 Parameters 324 PCA Comp 6 GMM Comp 6 Cov Matrix full exceeds 50 params\n",
      "165 Parameters 86 PCA Comp 6 GMM Comp 7 Cov Matrix spherical exceeds 50 params\n",
      "166 Parameters 168 PCA Comp 6 GMM Comp 7 Cov Matrix diag exceeds 50 params\n",
      "167 Parameters 126 PCA Comp 6 GMM Comp 7 Cov Matrix tied exceeds 50 params\n",
      "168 Parameters 378 PCA Comp 6 GMM Comp 7 Cov Matrix full exceeds 50 params\n",
      "169 Parameters 16 PCA Comp 7 GMM Comp 1 Cov Matrix spherical Accuracy is\t 61.53%\n",
      "170 Parameters 28 PCA Comp 7 GMM Comp 1 Cov Matrix diag Accuracy is\t 48.51%\n",
      "171 Parameters 70 PCA Comp 7 GMM Comp 1 Cov Matrix tied exceeds 50 params\n",
      "172 Parameters 70 PCA Comp 7 GMM Comp 1 Cov Matrix full exceeds 50 params\n",
      "173 Parameters 30 PCA Comp 7 GMM Comp 2 Cov Matrix spherical Accuracy is\t 61.53%\n",
      "174 Parameters 56 PCA Comp 7 GMM Comp 2 Cov Matrix diag exceeds 50 params\n",
      "175 Parameters 84 PCA Comp 7 GMM Comp 2 Cov Matrix tied exceeds 50 params\n",
      "176 Parameters 140 PCA Comp 7 GMM Comp 2 Cov Matrix full exceeds 50 params\n",
      "177 Parameters 44 PCA Comp 7 GMM Comp 3 Cov Matrix spherical Accuracy is\t 61.53%\n",
      "178 Parameters 84 PCA Comp 7 GMM Comp 3 Cov Matrix diag exceeds 50 params\n",
      "179 Parameters 98 PCA Comp 7 GMM Comp 3 Cov Matrix tied exceeds 50 params\n",
      "180 Parameters 210 PCA Comp 7 GMM Comp 3 Cov Matrix full exceeds 50 params\n",
      "181 Parameters 58 PCA Comp 7 GMM Comp 4 Cov Matrix spherical exceeds 50 params\n",
      "182 Parameters 112 PCA Comp 7 GMM Comp 4 Cov Matrix diag exceeds 50 params\n",
      "183 Parameters 112 PCA Comp 7 GMM Comp 4 Cov Matrix tied exceeds 50 params\n",
      "184 Parameters 280 PCA Comp 7 GMM Comp 4 Cov Matrix full exceeds 50 params\n",
      "185 Parameters 72 PCA Comp 7 GMM Comp 5 Cov Matrix spherical exceeds 50 params\n",
      "186 Parameters 140 PCA Comp 7 GMM Comp 5 Cov Matrix diag exceeds 50 params\n",
      "187 Parameters 126 PCA Comp 7 GMM Comp 5 Cov Matrix tied exceeds 50 params\n",
      "188 Parameters 350 PCA Comp 7 GMM Comp 5 Cov Matrix full exceeds 50 params\n",
      "189 Parameters 86 PCA Comp 7 GMM Comp 6 Cov Matrix spherical exceeds 50 params\n",
      "190 Parameters 168 PCA Comp 7 GMM Comp 6 Cov Matrix diag exceeds 50 params\n",
      "191 Parameters 140 PCA Comp 7 GMM Comp 6 Cov Matrix tied exceeds 50 params\n",
      "192 Parameters 420 PCA Comp 7 GMM Comp 6 Cov Matrix full exceeds 50 params\n",
      "193 Parameters 100 PCA Comp 7 GMM Comp 7 Cov Matrix spherical exceeds 50 params\n",
      "194 Parameters 196 PCA Comp 7 GMM Comp 7 Cov Matrix diag exceeds 50 params\n",
      "195 Parameters 154 PCA Comp 7 GMM Comp 7 Cov Matrix tied exceeds 50 params\n",
      "196 Parameters 490 PCA Comp 7 GMM Comp 7 Cov Matrix full exceeds 50 params\n",
      "\n",
      " The best categories: \n",
      " PC-1           pics\n",
      "PC-2         midday\n",
      "PC-3     reddit.com\n",
      "PC-4           july\n",
      "PC-5      worldnews\n",
      "PC-6        morning\n",
      "PC-7          trees\n",
      "PC-8         gaming\n",
      "PC-9          trees\n",
      "PC-10        videos\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def calc_param(n_features, n_components, covtype, classes):\n",
    "## Function to compute the total parameters for model\n",
    "   if covtype == 'full': #Covariance Parameters (n_components, n_features, n_features) Symmetric matrix\n",
    "        totparam = (n_features*n_components + (n_features*(n_features+1)/2)*n_components)*classes\n",
    "   elif covtype == 'diag': #Covariance Parameters (n_components, n_features)    \n",
    "        totparam = (n_features*n_components + n_features*n_components)*classes\n",
    "   elif covtype == 'spherical': #Covariance Parameters = (n_components,)\n",
    "        totparam = (n_features*n_components + 1)*classes\n",
    "   elif covtype == 'tied': #Covariance Parameters (n_features, n_features)  Symmetric matrix\n",
    "        totparam = (n_features*n_components + (n_features*(n_features+1)/2))*classes\n",
    "   else:\n",
    "      print('Invalid Covariance Type %s'%(covtype))\n",
    "    \n",
    "   return totparam\n",
    "\n",
    "\n",
    "cov_matrix = ['spherical', 'diag', 'tied', 'full'] #Initialize list of covariance types \n",
    "modelnum = 0 #Initialize model number\n",
    "#Loop through to run a series of experiments to find the model that gives the best accuracy with no more than 50 parameters\n",
    "for comp in range(1,8): #Vary the Principal components between 1 and 8\n",
    "    #Project the train data onto the reduced Principal component subspace\n",
    "    sklearn_pca = PCA(n_components=comp)\n",
    "    sklearn_transf = sklearn_pca.fit_transform(bincat_train_data)\n",
    "    #Divide the projected data into positive and negative class datasets\n",
    "    positive_train_data = sklearn_transf[bincat_train_labels == True]\n",
    "    negative_train_data = sklearn_transf[bincat_train_labels == False]\n",
    "\n",
    "    for GMMcomp in range(1,8): #Vary the Gaussian Mixture model components between 1 and 8\n",
    "       for covtype in cov_matrix: #Loop thru the 4 covariance types for the PCA and GMM components\n",
    "        totparam = calc_param(comp, GMMcomp, covtype, 2) #Calculate the total parameters for model\n",
    "        modelnum += 1 #Increment the model number\n",
    "        if totparam <= 50: #Validate that the total parameters for model is <= 50\n",
    "        #Train the GMM positive and negative models on the positive and negative datasets respectively\n",
    "               gm_mod_pos = GMM(n_components = GMMcomp, covariance_type=covtype)\n",
    "               gm_mod_pos.fit(positive_train_data)\n",
    "               gm_mod_neg = GMM(n_components = GMMcomp, covariance_type=covtype)\n",
    "               gm_mod_neg.fit(negative_train_data)\n",
    "        #Score the trained model on the test dataset\n",
    "               sklearn_test_transf = sklearn_pca.transform(bincat_test_data)\n",
    "               pscore = gm_mod_pos.score(sklearn_test_transf)\n",
    "               nscore = gm_mod_neg.score(sklearn_test_transf)\n",
    "\n",
    "        #Loop to determine the predicted labels of the test data based on the trained model\n",
    "               preds = []\n",
    "               for pos, neg in zip(pscore, nscore):\n",
    "                    if pos > neg: preds.append(1)\n",
    "                    else: preds.append(0)\n",
    "\n",
    "        #Loop to compare the predicted label with the actual label to determine the no of correct answers\n",
    "               correct = 0\n",
    "               for pred, label in zip(preds, bincat_test_labels):\n",
    "                    if pred == label: correct +=1\n",
    "        #Calculate and print the accuracy of the model\n",
    "               print('%d Parameters %d PCA Comp %d GMM Comp %d Cov Matrix %s Accuracy is\\t %.2f%%'% (modelnum, totparam, comp, GMMcomp, covtype, (100.0*correct/len(bincat_test_labels))))\n",
    "        else: #If total parameters > 50 skip\n",
    "               print('%d Parameters %d PCA Comp %d GMM Comp %d Cov Matrix %s exceeds 50 params'% (modelnum, totparam, comp, GMMcomp, covtype))\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit_transform(bincat_train_data)\n",
    "ind = ['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9','PC-10']\n",
    "\n",
    "# Dump components relations with features:\n",
    "\n",
    "pca_df = pd.DataFrame(pca.components_,columns=bincat_train_data.columns,index = ind)\n",
    "\n",
    "\n",
    "\n",
    "best_cat_cols = pca_df.idxmax(axis=1)\n",
    "\n",
    "print(\"\\n The best categories: \\n\", best_cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Looking at Numeric Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression Model \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.99      0.86      1532\n",
      "       True       0.63      0.03      0.07       488\n",
      "\n",
      "avg / total       0.73      0.76      0.67      2020\n",
      "\n",
      "Accuracy Score for this model =  0.761881188119\n",
      "\n",
      "\n",
      "\n",
      "Results of Random Forest Implementation \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.92      0.84      1532\n",
      "       True       0.35      0.13      0.19       488\n",
      "\n",
      "avg / total       0.67      0.73      0.68      2020\n",
      "\n",
      "Accuracy Score for this model =  0.731683168317\n",
      "\n",
      "\n",
      "\n",
      "Results of XGBoost Implementation \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.77      0.98      0.86      1532\n",
      "       True       0.48      0.06      0.11       488\n",
      "\n",
      "avg / total       0.70      0.76      0.68      2020\n",
      "\n",
      "Accuracy Score for this model =  0.757425742574\n"
     ]
    }
   ],
   "source": [
    "# randomizing the data (if we want to do this)\n",
    "#shuffle = np.random.permutation(np.arange(num_data.shape[0]))\n",
    "#num_train_data, num_train_labels = num_data[shuffle], num_labels[shuffle]\n",
    "\n",
    "# split for training/development\n",
    "num_train_data = num_data[0:2019]\n",
    "num_train_labels = train_labels[0:2019]\n",
    "num_test_data = num_data[2020:]\n",
    "num_test_labels = train_labels[2020:]\n",
    "\n",
    "\n",
    "# simple logistic regression model\n",
    "lf = LogisticRegression(C = 100)\n",
    "lf.fit(num_train_data, num_train_labels)\n",
    "\n",
    "preds = lf.predict(num_test_data)\n",
    "\n",
    "print(\"Results of Logistic Regression Model \\n\", classification_report(num_test_labels,preds))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# simple Random Forests\n",
    "lf2 = RandomForestClassifier(n_estimators=12)\n",
    "lf2.fit(num_train_data, num_train_labels)\n",
    "\n",
    "preds2 = lf2.predict(num_test_data)\n",
    "\n",
    "print(\"Results of Random Forest Implementation \\n\", classification_report(num_test_labels,preds2))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds2))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# trying a boosting algorithm\n",
    "gbm = xgb.XGBClassifier()\n",
    "gbm.fit(num_train_data, num_train_labels)\n",
    "preds3 = gbm.predict(num_test_data)\n",
    "\n",
    "print(\"Results of XGBoost Implementation \\n\", classification_report(num_test_labels,preds3))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Random Forest Implementation \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.94      0.84      1532\n",
      "       True       0.23      0.05      0.09       488\n",
      "\n",
      "avg / total       0.63      0.73      0.66      2020\n",
      "\n",
      "Accuracy Score for this model =  0.728217821782\n"
     ]
    }
   ],
   "source": [
    "# trying RandomForest\n",
    "clf2 = RandomForestClassifier(n_estimators=12)\n",
    "clf2.fit(bincat_train_data, bincat_train_labels)\n",
    "\n",
    "preds2 = clf2.predict(bincat_test_data)\n",
    "\n",
    "print(\"Results of Random Forest Implementation \\n\", classification_report(num_test_labels,preds2))\n",
    "print(\"Accuracy Score for this model = \", metrics.accuracy_score(num_test_labels,preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is very similar to the numerical data by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'request_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2133\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'request_length'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-561c89aab96e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m## Considering the effect of the length of a post\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m print('Average Length of Granted Request:', np.average(text_data[text_data['requester_received_pizza'] == True]['request_length']),\n\u001b[0m\u001b[1;32m     11\u001b[0m       '\\nAverage Length of Failed Request:', np.average(text_data[text_data['requester_received_pizza'] == False]['request_length']))\n\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\skarb\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'request_length'"
     ]
    }
   ],
   "source": [
    "### Pull in text data\n",
    "a = pd.read_json('train.json', orient='columns')\n",
    "data_set = a[['request_text_edit_aware','request_title']]\n",
    "text_data = pd.concat([df6_labels, data_set], axis = 1) \n",
    "data_labels = a['requester_received_pizza']\n",
    "\n",
    "###########################################################################################\n",
    "## Considering the effect of the length of a post\n",
    "\n",
    "print('Average Length of Granted Request:', np.average(text_data[text_data['requester_received_pizza'] == True]['request_length']),\n",
    "      '\\nAverage Length of Failed Request:', np.average(text_data[text_data['requester_received_pizza'] == False]['request_length']))\n",
    "\n",
    "bunny = text_data[text_data['requester_received_pizza'] == True]\n",
    "bunny['request_length'] = bunny['request_text_edit_aware'].map(lambda x: len(x.split()))\n",
    "np.average(bunny['request_length'])\n",
    "\n",
    "jackal = text_data[text_data['requester_received_pizza'] == False]\n",
    "jackal['request_length'] = jackal['request_text_edit_aware'].map(lambda x: len(x.split()))\n",
    "np.average(jackal['request_length'])\n",
    "\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "#x = pd.Series(bunny['request_length'], name = 'request granted lenth')\n",
    "#ax = sns.distplot(x)\n",
    "sns.set(rc={\"figure.figsize\": (8, 5)});\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(bunny['request_length'], ax=ax, color='green',kde=False, rug=False,bins=200)\n",
    "sns.distplot(jackal['request_length'], ax=ax, color='red',kde=False, rug=False,bins=200)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Distributions of Request Lengths, By Success Status')\n",
    "plt.legend(loc='upper left')\n",
    "sns.plt.show()\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"Use a series of regex expressions to remove unwanted characters\"\"\"\n",
    "    #remove non-alpha-numeric characters, replace with whitespace\n",
    "    x1 = re.sub(r'[^a-zA-Z_0-9_\\s]',\" \", x).lower()\n",
    "    #replae all numbers with a single token and a space afterwards\n",
    "    x1a = re.sub(r'[0-9]+', 'number ', x1)\n",
    "    #x1b = re.sub(r'[_]+', ' ', x1a)\n",
    "    #even though there are words that are just '_____', f1 actuall decreases when they're removed\n",
    "    #remove newlines\n",
    "    x2 = re.sub(r'[\\n]', \" \", x1a)\n",
    "    #scrub out extra spaces\n",
    "    x3 = re.sub(r'\\s+', ' ', x2)  #other steps might have added extra space; remove\n",
    "    return x3.strip()\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size=0.25, random_state=1)\n",
    "for train_index, test_index in sss.split(data_set, data_labels):\n",
    "    X_train, X_test = data_set.iloc[train_index], data_set.iloc[test_index]\n",
    "    y_train, y_test = data_labels.iloc[train_index], data_labels.iloc[test_index]\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "# Feature Union for post title and post text, w/ logistic regression\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    #('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('get-title', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_title')),\n",
    "                ('cv', TfidfVectorizer(analyzer='char_wb', \n",
    "                                       #vocabulary=vocabulary,\n",
    "                                       preprocessor=preprocess,\n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('get-request', Pipeline([\n",
    "                ('selector', ItemSelector(key='request_text_edit_aware')),\n",
    "                ('cv', TfidfVectorizer(analyzer='char_wb', \n",
    "                                       #vocabulary=vocabulary,\n",
    "                                       preprocessor=preprocess,\n",
    "                                       max_df=0.5, ngram_range=(1,3)))\n",
    "            ])),\n",
    "            \n",
    "           #('get-number', Pipeline([\n",
    "           #     ('selector', ItemSelector(key='number'))\n",
    "           # ]))\n",
    "\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('nb',LogisticRegression(penalty='l2',C=100)),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "text_pred = pipeline.predict(X_test)\n",
    "target_names = [ 'No pizza','Got pizza']\n",
    "print(\"Results of Logistic Regression on Text Data \\n\", classification_report(y_test, text_pred, target_names=target_names))\n",
    "\n",
    "#top_word_pairs = get_top_words(text_data['requester_received_pizza'], tkn2_request, fit2_req)\n",
    "#print(\"\\nPaired Word Frequency Matrix: Top Words per Article Type \\n\\t\",', '.join(newsgroups_train.target_names))\n",
    "#top_term_pairs = get_top_terms(top_word_pairs[0], top_word_pairs[1], fit2_req, text_data['requester_received_pizza'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning for Ada Boost\n",
    "\n",
    "The module sklearn.ensemble includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995].\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights w_1, w_2, ..., w_N to each of the training samples. Initially, those weights are all set to w_i = 1/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence [HTF]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for AdaBoost Implementation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.78      0.91      0.84       609\n",
      "  Got pizza       0.46      0.23      0.31       199\n",
      "\n",
      "avg / total       0.71      0.75      0.71       808\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_norm_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-95564e2c245c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfeature_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntstdf6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         csr_matrix(num_norm_test)), \n\u001b[0m\u001b[1;32m     65\u001b[0m         format='csr')\n\u001b[1;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_norm_test' is not defined"
     ]
    }
   ],
   "source": [
    "#best kaggle score\n",
    "### add a stemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "### change the binarized test data to contain the same columns as the train data\n",
    "#df6_cols = [x for x in df6_data.columns if x in tstdf6.columns]\n",
    "ntstdf6 = pd.DataFrame(index=tstdf5.index)\n",
    "for x in list(df6_data.columns):\n",
    "    if x in tstdf5.columns:\n",
    "        ntstdf6[x] = tstdf5[x]\n",
    "    else:\n",
    "        ntstdf6[x] = 0\n",
    "ntstdf6.shape\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "tkn_request = TfidfVectorizer(preprocessor=preprocess,\n",
    "                              stop_words='english',\n",
    "                              #vocabulary=good_words,\n",
    "                             analyzer = stemmed_words)\n",
    "tkn_title = TfidfVectorizer(preprocessor=preprocess,\n",
    "                            stop_words='english',\n",
    "                           analyzer = stemmed_words)\n",
    "feature_request = tkn_request.fit_transform(text_data['request_text_edit_aware'])\n",
    "feature_title = tkn_title.fit_transform(text_data['request_title'])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "X = hstack((\n",
    "        csr_matrix(feature_request),\n",
    "        csr_matrix(feature_title),\n",
    "        csr_matrix(df6_data), \n",
    "        csr_matrix(num_norm)), \n",
    "        format='csr')\n",
    "\n",
    "y = text_data['requester_received_pizza']\n",
    "\n",
    "# shuffle data\n",
    "sss = StratifiedShuffleSplit(test_size=\n",
    "                             0.20, random_state=1)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "# implement ada boost classifier\n",
    "ad2 = AdaBoostClassifier()\n",
    "ad2.fit(X_train, y_train)\n",
    "ad2_pred = ad2.predict(X_test)\n",
    "print(\"Results for AdaBoost Implementation: \\n\", classification_report(y_test, ad2_pred, target_names=target_names))\n",
    "\n",
    "# consider if number of answered requests for pizza is similiar to train data\n",
    "np.average(Y_pred)\n",
    "\n",
    "# put in proper submission format\n",
    "Y_pred_2 = vint(Y_pred)\n",
    "fin_df = pd.concat([test['request_id'],pd.DataFrame(Y_pred_2, columns=['requester_received_pizza'])],axis =1)\n",
    "fin_df['requester_received_pizza'] = pd.to_numeric(fin_df['requester_received_pizza'],downcast='signed')\n",
    "\n",
    "# verify format visually\n",
    "fin_df.head()\n",
    "\n",
    "# output to file\n",
    "fin_df.to_csv('pizza_submission11.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[556,  53],\n",
       "       [153,  46]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true = y_test, y_pred =ad2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Appendix\n",
    "\n",
    "Here you will miscellaneous techniques performed that did not necessarily inform the decision of the final model. This section may not be as organized as the rest of the notebook, but is included for posterity and possible usefulness in any future analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bunch of random model tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.92      1.00      0.96      3046\n",
      "  Got pizza       1.00      0.72      0.84       994\n",
      "\n",
      "avg / total       0.94      0.93      0.93      4040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = LogisticRegression()\n",
    "model1.fit(X, y)\n",
    "y_pred = model1.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       0.77      0.78      0.77      1006\n",
      "  Got pizza       0.29      0.28      0.28       328\n",
      "\n",
      "avg / total       0.65      0.66      0.65      1334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(test_size=0.33, random_state=1)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "modeldt = DecisionTreeClassifier()\n",
    "modeldt.fit(X_train, y_train)\n",
    "y_pred = modeldt.predict(X_test)\n",
    "target_names = [ 'No pizza','Got pizza']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   No pizza       1.00      1.00      1.00      3046\n",
      "  Got pizza       1.00      1.00      1.00       994\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4040\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
